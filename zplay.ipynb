{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latency': 0.005711685982532799, 'ttft': 0.005710346973501146, 'time_per_token': 0.003279856679728255, 'throughput': 302.61582927922353}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "def test_gpt2_inference_metrics(model_name=\"gpt2\", prompt=\"Hello, how are you?\", max_tokens=50):\n",
    "    \"\"\"\n",
    "    Test inference metrics for GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the GPT-2 model to load.\n",
    "        prompt (str): Input text to feed into the model.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics including latency, TTFT, time per token, and throughput.\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = time.perf_counter()\n",
    "    generated_tokens = []\n",
    "\n",
    "    # Generate tokens and record times\n",
    "    latency = None\n",
    "    ttft = None\n",
    "    token_times = []\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        # Measure time for the first token\n",
    "        if i == 0:\n",
    "            token_start_time = time.perf_counter()\n",
    "            output = model.generate(input_ids, max_new_tokens=1, do_sample=False)\n",
    "            token_end_time = time.perf_counter()\n",
    "            if latency is None:\n",
    "                latency = token_end_time - start_time\n",
    "                ttft = token_end_time - token_start_time\n",
    "        else:\n",
    "            # Generate one token at a time\n",
    "            token_start_time = time.perf_counter()\n",
    "            output = model.generate(input_ids, max_new_tokens=1, do_sample=False)\n",
    "            token_end_time = time.perf_counter()\n",
    "\n",
    "        # Record time per token\n",
    "        token_times.append(token_end_time - token_start_time)\n",
    "\n",
    "        # Update input_ids with newly generated token\n",
    "        input_ids = torch.cat((input_ids, output[:, -1:]), dim=1)\n",
    "        generated_tokens.append(output[:, -1:].item())\n",
    "\n",
    "    # Final time\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Metrics calculation\n",
    "    total_time = end_time - start_time\n",
    "    time_per_token = sum(token_times) / len(token_times)\n",
    "    throughput = len(generated_tokens) / total_time\n",
    "\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"ttft\": ttft,\n",
    "        \"time_per_token\": time_per_token,\n",
    "        \"throughput\": throughput,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    metrics = test_gpt2_inference_metrics(prompt=\"Once upon a time, in a faraway land,\", max_tokens=10)\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Insert this phrase of 'In the meanwhile,' in to this sentence of 'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji.'. The inserted sentence is: 'In the meanwhile, Narayan Dhurmaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji.'\n",
      "\n",
      "The word 'in' is not a part of the sentence. It is inserted by the writer.\n",
      "\n",
      "The word 'in' is not a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "def generate_code_fix(prompt, model_name=\"gpt2-xl\", max_tokens=20, seed=42):\n",
    "    \"\"\"\n",
    "    Generate the next tokens using GPT-2 XL given a code prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to provide to the model.\n",
    "        model_name (str): The name of the GPT-2 model to use (default is \"gpt2-xl\").\n",
    "        max_tokens (int): The number of tokens to generate (default is 100).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Set pad_token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input prompt and create the attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)  # Explicit attention mask\n",
    "\n",
    "    # Generate the next tokens\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Pass attention mask\n",
    "        max_new_tokens=len(input_ids[0]) + max_tokens,  # Maximum number of tokens to generate\n",
    "        # do_sample = True,\n",
    "        pad_token_id=tokenizer.pad_token_id  # Handle padding explicitly\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# code = \"\"\"def bitcount(n):\n",
    "#     count = 0\n",
    "#     while n:\n",
    "#         n ^= n - 1\n",
    "#         count += 1\n",
    "#     return count\"\"\"\n",
    "\n",
    "# prompt = f\"Given the following code is incorrect:\\n{code}\\nCorrected code:\"\n",
    "# result = generate_code_fix(prompt, seed=56)\n",
    "# print(\"Generated Text:\")\n",
    "# print(result)\n",
    "\n",
    "base = 'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji .'\n",
    "phrase = 'In the meanwhile ,'\n",
    "prompt = f\"Insert this phrase of '{phrase}' in to this sentence of '{base}'. The inserted sentence is:\"\n",
    "result = generate_code_fix(prompt, seed=56)\n",
    "print(\"Generated Text:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Given this bugged code: \n",
      " def bitcount(n):\n",
      "    count = 0\n",
      "    while n:\n",
      "        n ^= n - 1\n",
      "        count += 1\n",
      "    return count. \n",
      " Fix the code above: \n",
      "def bitcount(n):\n",
      "      n = int(n)\n",
      "      n ^= n - 1\n",
      "      count = 0\n",
      "      while n:\n",
      "      n ^= n - 1\n",
      "      count += 1\n",
      "      return count. \n",
      "The first line of the above program prints \"0\" to the console. The second line prints \"1\" to the console. What the program doesn't know is\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"def bitcount(n):\n",
    "    count = 0\n",
    "    while n:\n",
    "        n ^= n - 1\n",
    "        count += 1\n",
    "    return count\"\"\"\n",
    "\n",
    "prompt = f\"Given this bugged code: \\n {code}. \\n Fix the code above: \"\n",
    "result = generate_code_fix(prompt)\n",
    "print(\"Generated Text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Extract sentences as summary: \n",
      " An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour. \n",
      "Extractive summarization: \n",
      "The Veendam, a cruise ship owned by Holland America, docked at Rio de Janeiro on Tuesday. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The Veendam left New York 36 days ago for a South America tour. \n",
      "Extract sentences as summary: \n",
      "The Veendam, a cruise ship owned by Holland America, docked at Rio de Janeiro on Tuesday. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The Veendam left\n"
     ]
    }
   ],
   "source": [
    "doc = \"\"\"An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour\"\"\"\n",
    "\n",
    "prompt = f\"Extract sentences as summary: \\n {doc}. \\nExtractive summarization: \"\n",
    "\n",
    "result = generate_code_fix(prompt)\n",
    "print(\"Generated Text:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
