{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latency': 0.005711685982532799, 'ttft': 0.005710346973501146, 'time_per_token': 0.003279856679728255, 'throughput': 302.61582927922353}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "def test_gpt2_inference_metrics(model_name=\"gpt2\", prompt=\"Hello, how are you?\", max_tokens=50):\n",
    "    \"\"\"\n",
    "    Test inference metrics for GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the GPT-2 model to load.\n",
    "        prompt (str): Input text to feed into the model.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics including latency, TTFT, time per token, and throughput.\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = time.perf_counter()\n",
    "    generated_tokens = []\n",
    "\n",
    "    # Generate tokens and record times\n",
    "    latency = None\n",
    "    ttft = None\n",
    "    token_times = []\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        # Measure time for the first token\n",
    "        if i == 0:\n",
    "            token_start_time = time.perf_counter()\n",
    "            output = model.generate(input_ids, max_new_tokens=1, do_sample=False)\n",
    "            token_end_time = time.perf_counter()\n",
    "            if latency is None:\n",
    "                latency = token_end_time - start_time\n",
    "                ttft = token_end_time - token_start_time\n",
    "        else:\n",
    "            # Generate one token at a time\n",
    "            token_start_time = time.perf_counter()\n",
    "            output = model.generate(input_ids, max_new_tokens=1, do_sample=False)\n",
    "            token_end_time = time.perf_counter()\n",
    "\n",
    "        # Record time per token\n",
    "        token_times.append(token_end_time - token_start_time)\n",
    "\n",
    "        # Update input_ids with newly generated token\n",
    "        input_ids = torch.cat((input_ids, output[:, -1:]), dim=1)\n",
    "        generated_tokens.append(output[:, -1:].item())\n",
    "\n",
    "    # Final time\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    # Metrics calculation\n",
    "    total_time = end_time - start_time\n",
    "    time_per_token = sum(token_times) / len(token_times)\n",
    "    throughput = len(generated_tokens) / total_time\n",
    "\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"ttft\": ttft,\n",
    "        \"time_per_token\": time_per_token,\n",
    "        \"throughput\": throughput,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    metrics = test_gpt2_inference_metrics(prompt=\"Once upon a time, in a faraway land,\", max_tokens=10)\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Insert this phrase of 'In the meanwhile,' in to this sentence of 'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji.'. The inserted sentence is: 'In the meanwhile, Narayan Dhurmaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji.'\n",
      "\n",
      "The word 'in' is not a part of the sentence. It is inserted by the writer.\n",
      "\n",
      "The word 'in' is not a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "def generate_code_fix(prompt, model_name=\"gpt2-xl\", max_tokens=20, seed=42):\n",
    "    \"\"\"\n",
    "    Generate the next tokens using GPT-2 XL given a code prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to provide to the model.\n",
    "        model_name (str): The name of the GPT-2 model to use (default is \"gpt2-xl\").\n",
    "        max_tokens (int): The number of tokens to generate (default is 100).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Set pad_token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize the input prompt and create the attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)  # Explicit attention mask\n",
    "\n",
    "    # Generate the next tokens\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Pass attention mask\n",
    "        max_new_tokens=len(input_ids[0]) + max_tokens,  # Maximum number of tokens to generate\n",
    "        # do_sample = True,\n",
    "        pad_token_id=tokenizer.pad_token_id  # Handle padding explicitly\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# code = \"\"\"def bitcount(n):\n",
    "#     count = 0\n",
    "#     while n:\n",
    "#         n ^= n - 1\n",
    "#         count += 1\n",
    "#     return count\"\"\"\n",
    "\n",
    "# prompt = f\"Given the following code is incorrect:\\n{code}\\nCorrected code:\"\n",
    "# result = generate_code_fix(prompt, seed=56)\n",
    "# print(\"Generated Text:\")\n",
    "# print(result)\n",
    "\n",
    "base = 'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji .'\n",
    "phrase = 'In the meanwhile ,'\n",
    "prompt = f\"Insert this phrase of '{phrase}' in to this sentence of '{base}'. The inserted sentence is:\"\n",
    "result = generate_code_fix(prompt, seed=56)\n",
    "print(\"Generated Text:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Given this bugged code: \n",
      " def bitcount(n):\n",
      "    count = 0\n",
      "    while n:\n",
      "        n ^= n - 1\n",
      "        count += 1\n",
      "    return count. \n",
      " Fix the code above: \n",
      "def bitcount(n):\n",
      "      n = int(n)\n",
      "      n ^= n - 1\n",
      "      count = 0\n",
      "      while n:\n",
      "      n ^= n - 1\n",
      "      count += 1\n",
      "      return count. \n",
      "The first line of the above program prints \"0\" to the console. The second line prints \"1\" to the console. What the program doesn't know is\n"
     ]
    }
   ],
   "source": [
    "char_edit\n",
    "acc:  0.0\n",
    "acc_all: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "THE WORD:   the\n",
    "Unpatched:\n",
    "[\"Please fix grammar of the following text: 'Crowley 's Ridge ( also Crowleys Ridge ) is an unusual alien formed geological formation that rises 250 to above tbhe alluvial plain of the Mississippi embayment in a line from southeastern Missouri to the Mississippi River near Helena , Arkansas .\\n'. The correct text is: Crowley 's Ridge ( also Crowleys Ridge ) is an unusual alien formed geological formation that rises 250 to above the alluvial plain of the Mississippi embayment in a line from southeastern Missouri to the Mississippi River near\"]\n",
    "Patched:\n",
    "[\"Please fix grammar of the following text: 'Crowley 's Ridge ( also Crowleys Ridge ) is an unusual alien formed geological formation that rises 250 to above tbhe alluvial plain of the Mississippi embayment in a line from southeastern Missouri to the Mississippi River near Helena , Arkansas .\\n'. The correct text is: Crowley 's Ridge ( also Crowleys Ridge ) is an unusual alien formed geological formation that rises 250 to above tbhe alluvial plain of the Mississippi embayment in a line from southeastern Missouri to the Mississippi River\"]\n",
    "-------------------------------\n",
    "acc:  0.0\n",
    "acc_all: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "THE WORD:   Second\n",
    "Unpatched:\n",
    "[\"Please fix grammar of the following text: 'Atlético Clube de Portugal are a Portuguese football team from Alcântara , Lisbon , that currently play in the Portuguese Sebcond Division - Serie D. They defeated F.C. Porto in the 2007 Taça de Portugal .\\n'. The correct text is: Atlético Clube de Portugal are a Portuguese football team from Alcântara , Lisbon , that currently play in the Portuguese Second Division - Serie D. They defeated F.C. Porto in the 2007 Taça\"]\n",
    "Patched:\n",
    "[\"Please fix grammar of the following text: 'Atlético Clube de Portugal are a Portuguese football team from Alcântara , Lisbon , that currently play in the Portuguese Sebcond Division - Serie D. They defeated F.C. Porto in the 2007 Taça de Portugal .\\n'. The correct text is: Atlético Clube de Portugal are a Portuguese football team from Alcântara , Lisbon , that currently play in the Portuguese Sebcond Division - Serie D. They defeated F.C. Porto in the 2007\"]\n",
    "-------------------------------\n",
    "acc:  0.23809523809523808\n",
    "acc_all: [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "THE WORD:   means\n",
    "Unpatched:\n",
    "[\"Please fix grammar of the following text: ''' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which maens the '' final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to merge technologically into the Mesolithic '' .\\n'. The correct text is: '' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which means the '' final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to\"]\n",
    "Patched:\n",
    "[\"Please fix grammar of the following text: ''' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which maens the '' final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to merge technologically into the Mesolithic '' .\\n'. The correct text is: '' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which means the '' final Upper Paleolithic industries occurring at the end of the final glaciation which appear to merge\"]\n",
    "-------------------------------\n",
    "acc:  0.23809523809523808\n",
    "acc_all: [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "THE WORD:   means\n",
    "Unpatched:\n",
    "[\"Please fix grammar of the following text: ''' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which mans the '' final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to merge technologically into the Mesolithic '' .\\n'. The correct text is: '' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which means the '' final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to\"]\n",
    "Patched:\n",
    "[\"Please fix grammar of the following text: ''' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which mans the '' final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to merge technologically into the Mesolithic '' .\\n'. The correct text is: '' 52 '' The term '' Mesolithic '' is in competition with another term , '' Epipaleolithic '' , which means the '' final Upper Paleolithic industries occurring at the end of the final glaciation which appear to merge\"]\n",
    "-------------------------------\n",
    "acc:  0.2857142857142857\n",
    "acc_all: [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "THE WORD:   given\n",
    "Unpatched:\n",
    "[\"Please fix grammar of the following text: 'When John Armstrong -- the only similarly qualified and more senior Navy captain -- was pronounced unfit for seagoing duty , Dowling was gvien the chance to command Australia 's first aircraft carrier , , commissioned in England on 16 December 1948 .\\n'. The correct text is: When John Armstrong -- the only similarly qualified and more senior Navy captain -- was pronounced unfit for seagoing duty , Dowling was given the chance to command Australia's first aircraft carrier, HMAS Sydney, commissioned in England on 1\"]\n",
    "Patched:\n",
    "[\"Please fix grammar of the following text: 'When John Armstrong -- the only similarly qualified and more senior Navy captain -- was pronounced unfit for seagoing duty , Dowling was gvien the chance to command Australia 's first aircraft carrier , , commissioned in England on 16 December 1948 .\\n'. The correct text is: When John Armstrong -- the only similarly qualified and more senior Navy captain -- was pronounced unfit for seagoing duty , Dowling was given the chance to command Australia 's first aircraft carrier , HMAS Sydney , commissioned in England on \"]\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Extract sentences as summary: \n",
      " An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour. \n",
      "Extractive summarization: \n",
      "The Veendam, a cruise ship owned by Holland America, docked at Rio de Janeiro on Tuesday. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The Veendam left New York 36 days ago for a South America tour. \n",
      "Extract sentences as summary: \n",
      "The Veendam, a cruise ship owned by Holland America, docked at Rio de Janeiro on Tuesday. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according to Agencia Brasil. The Veendam left\n"
     ]
    }
   ],
   "source": [
    "doc = \"\"\"An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour\"\"\"\n",
    "\n",
    "prompt = f\"Extract sentences as summary: \\n {doc}. \\nExtractive summarization: \"\n",
    "\n",
    "result = generate_code_fix(prompt)\n",
    "print(\"Generated Text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Insert this phrase of 'In the meanwhile ,' in to this sentence of 'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji .'. The inserted sentence is: 'In the meanwhile, Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji.'\\n\\nInsert this phrase of 'In the meanwhile,' in to this sentence of 'Dadaji lost his mother and\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=\"facebook/opt-2.7b\", max_new_tokens = 50, device='cuda:0')\n",
    "\n",
    "\n",
    "base = 'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji .'\n",
    "phrase = 'In the meanwhile ,'\n",
    "prompt = f\"Insert this phrase of '{phrase}' in to this sentence of '{base}'. The inserted sentence is:\"\n",
    "\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 44, but `max_length` is set to 21. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m phrase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn the meanwhile ,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsert this phrase of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphrase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in to this sentence of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. The inserted sentence is:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/pipelines/base.py:1264\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/pipelines/base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:351\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/generation/utils.py:1874\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1867\u001b[0m         model_kwargs[cache_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1868\u001b[0m             DynamicCache\u001b[38;5;241m.\u001b[39mfrom_legacy_cache(past)\n\u001b[1;32m   1869\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m requires_cross_attention_cache\n\u001b[1;32m   1870\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m EncoderDecoderCache\u001b[38;5;241m.\u001b[39mfrom_legacy_cache(past)\n\u001b[1;32m   1871\u001b[0m         )\n\u001b[1;32m   1872\u001b[0m         use_dynamic_cache_by_default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/generation/utils.py:1266\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1265\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1270\u001b[0m     )\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1276\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 44, but `max_length` is set to 21. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
