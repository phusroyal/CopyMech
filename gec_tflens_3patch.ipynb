{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_top_k(logits, top_k=5):\n",
    "    \"\"\"\n",
    "    logits: (B, T, vocab_size)\n",
    "    Returns a list of top-k token IDs for the last position, e.g. [id1, id2,...].\n",
    "    \"\"\"\n",
    "    last_logits = logits[:, -1, :]       # shape (B, vocab_size)\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    top_vals, top_indices = probs.topk(top_k, dim=-1)\n",
    "    # top_indices is shape (B, top_k). For B=1, we do top_indices[0].tolist().\n",
    "    return top_indices[0].tolist()\n",
    "\n",
    "def detect_ngram_copy(seq_ids: torch.Tensor, n=3):\n",
    "    \"\"\"\n",
    "    Minimal function that tries to find n-gram copy scenario\n",
    "    (just a placeholder – adapt to your real logic)\n",
    "    \"\"\"\n",
    "    T = seq_ids.size(1)  # shape (B=1, T)\n",
    "    if T < n:\n",
    "        return None, None\n",
    "    # 1) last token\n",
    "    last_token = seq_ids[0, -1].item()\n",
    "    # 2) find earlier positions of last_token\n",
    "    possible_pos = (seq_ids[0, :-1] == last_token).nonzero().view(-1)\n",
    "    if possible_pos.numel() == 0:\n",
    "        return None, None\n",
    "    # 3) check (n-1) context\n",
    "    n_minus_1 = n - 1\n",
    "    context_needed = seq_ids[0, -(n_minus_1+1):-1]  # last n-1 tokens\n",
    "    matched_pos = None\n",
    "    for pos in reversed(possible_pos):\n",
    "        if pos >= n_minus_1:\n",
    "            candidate = seq_ids[0, pos-n_minus_1:pos]\n",
    "            if torch.all(candidate == context_needed):\n",
    "                matched_pos = pos.item()\n",
    "                break\n",
    "    if matched_pos is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return  intersection / union\n",
    "\n",
    "\n",
    "def get_acc(info_lst):\n",
    "    jcc_ult = []\n",
    "    acc_ult = []\n",
    "\n",
    "    for data in info_lst:\n",
    "        acc_lst = []\n",
    "        jc_lst = []\n",
    "        for step in data.keys():\n",
    "            copy = data[step]['copy']\n",
    "            original = data[step]['original']\n",
    "\n",
    "            jaccard_score = jaccard_similarity(copy, original)\n",
    "            jc_lst.append(jaccard_score)\n",
    "\n",
    "            acc_score = 1 if copy[0] == original[0] else 0\n",
    "            acc_lst.append(acc_score)\n",
    "\n",
    "        jcc_ult.append(jc_lst)\n",
    "        acc_ult.append(acc_lst)\n",
    "    \n",
    "    def cal_avg(lsts):\n",
    "        avg_lst = []\n",
    "        for lst in lsts:\n",
    "            if len(lst) == 0:\n",
    "                continue\n",
    "            avg_lst.append(sum(lst) / len(lst))\n",
    "        return sum(avg_lst) / len(avg_lst)\n",
    "\n",
    "    avg_jcc = cal_avg(jcc_ult)\n",
    "    avg_acc = cal_avg(acc_ult)\n",
    "\n",
    "    return avg_jcc, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa8a847bfa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e1014bfc334202aa9ce97e10896b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-3B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, Qwen2Tokenizer\n",
    "\n",
    "\n",
    "# load model and tokenizer\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
    "# tokenizer = Qwen2Tokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def mini_acc(dict_pred_info):\n",
    "    acc_lst = []\n",
    "    for step in dict_pred_info.keys():\n",
    "        copy = dict_pred_info[step]['copy']\n",
    "        original = dict_pred_info[step]['original']\n",
    "\n",
    "        acc_score = 1 if copy[0] == original[0] else 0\n",
    "        acc_lst.append(acc_score)\n",
    "    return sum(acc_lst) / len(acc_lst)\n",
    "\n",
    "def has_required_spaces(seq: str, seqlen = 6) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the sequence has an occurrence of 'is' or 'are'\n",
    "    that is preceded (anywhere earlier in the sequence) by at least 6 tokens that are exactly 'space'.\n",
    "    \n",
    "    Examples:\n",
    "      'There space space space space space oh space is a cat.' -> True\n",
    "      'There space are many cats.' -> False\n",
    "      'There is a cat.' -> False\n",
    "      'There space space space space space space is a cat.' -> True\n",
    "      'There spaces are many cats.' -> False\n",
    "    \"\"\"\n",
    "    tokens = seq.split()\n",
    "    # check if sentence only has 1 is or are\n",
    "    if tokens.count(\"is\") + tokens.count(\"are\") != 1:\n",
    "        return False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in {\"is\", \"are\"}:\n",
    "            # Count how many tokens before this occurrence are exactly \"space\"\n",
    "            if len(tokens[:i]) >= seqlen:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\"Given a text, replace ' is ' by ' are ', and vice versa. Return the corrupted text, and the text until the first is/are.\"\"\"\n",
    "    text = text.strip()\n",
    "    if ' is ' in text:\n",
    "        corrupted_text = text.replace(' is ', ' are ', 1)\n",
    "    elif ' are ' in text:\n",
    "        corrupted_text = text.replace(' are ', ' is ', 1)\n",
    "    \n",
    "    # find position of first is/are and return text before that\n",
    "    first_is = text.find(' is ')\n",
    "    first_are = text.find(' are ')\n",
    "    if first_is == -1 and first_are == -1:\n",
    "        return None\n",
    "    elif first_is == -1:\n",
    "        return corrupted_text, text[:first_are], 'are'\n",
    "    elif first_are == -1:\n",
    "        return corrupted_text, text[:first_is], 'is'\n",
    "    \n",
    "    return corrupted_text, text[:min(first_is, first_are)]\n",
    "\n",
    "def ngram(n, model, skip_up_to, max_steps, extra_steps, k, edited_phrases):\n",
    "\n",
    "    print(\"n-gram: \", n)\n",
    "    print(\"Skip layers: \", skip_up_to)\n",
    "\n",
    "    info_lst = []\n",
    "    failed_lst = []\n",
    "\n",
    "    total_failed_by_prepocess = 0\n",
    "    total_failed_as_ulsolvable = 0\n",
    "    total_solvable_og = 0\n",
    "    total_solvable_pt = 0\n",
    "    total_matches = []\n",
    "\n",
    "    num_matched = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for edited in tqdm(edited_phrases):\n",
    "\n",
    "        if total_solvable_og == 100:\n",
    "            break\n",
    "\n",
    "        if not has_required_spaces(edited, seqlen=7):\n",
    "            continue\n",
    "\n",
    "        # preprocess text\n",
    "        edited = text_preprocess(edited)\n",
    "        if edited is None:\n",
    "            total_failed_by_prepocess += 1\n",
    "            continue\n",
    "        corrupted_text, pre_isare, correct_tobe = edited\n",
    "        prompt = f\"Please fix grammar of the following text: '{corrupted_text}'. The correct text is: {pre_isare}\"\n",
    "\n",
    "        # edited_ids = tokenizer.encode(edited, return_tensors='pt')\n",
    "        # steps = extra_steps + edited_ids.size(1)\n",
    "        # if steps > max_steps:\n",
    "        #     steps = max_steps\n",
    "        # total_steps += steps\n",
    "        steps = 1\n",
    "\n",
    "        dict_pred_info = defaultdict(dict)\n",
    "\n",
    "        for step_i in range(steps):\n",
    "            prompt_tokens = model.to_tokens(prompt)\n",
    "\n",
    "            if step_i != 0:\n",
    "                prompt_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
    "\n",
    "            # run on the prompt once with cache to store activations to patch in later\n",
    "            og_logits, og_cache = model.run_with_cache(prompt_tokens)\n",
    "            # get the top k tokens\n",
    "            og_topk_indices = get_top_k(og_logits, k)\n",
    "            # get the highest prob token\n",
    "            og_next_token = torch.tensor([og_topk_indices[0]]).unsqueeze(0).to(og_logits.device)\n",
    "\n",
    "            # check if model can solve the task\n",
    "            decoded_og_next_token = model.to_string(og_next_token)[0]\n",
    "            if 'are' in decoded_og_next_token and correct_tobe == 'are':\n",
    "                total_solvable_og += 1\n",
    "            elif 'is' in decoded_og_next_token and correct_tobe == 'is':\n",
    "                total_solvable_og += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # detect copy scenario\n",
    "            t_matched_1st = detect_ngram_copy(prompt_tokens, n=n)\n",
    "            t_matched_2nd = detect_ngram_copy(prompt_tokens[:,:-1], n=n)\n",
    "            t_matched_3nd = detect_ngram_copy(prompt_tokens[:,:-2], n=n)\n",
    "            \n",
    "            if t_matched_1st is not None and t_matched_2nd is not None:\n",
    "                pos_matched = [t_matched_1st, t_matched_2nd, t_matched_3nd]\n",
    "                pos_current = [len(prompt_tokens[0])-1, len(prompt_tokens[0])-2, len(prompt_tokens[0])-3]\n",
    "                \n",
    "                num_matched += 1\n",
    "\n",
    "                def residual_stream_patching_hook(\n",
    "                    resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "                    hook: HookPoint,\n",
    "                    pos_matched: list,\n",
    "                    pos_current: list\n",
    "                ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "                    # Each HookPoint has a name attribute giving the name of the hook.\n",
    "                    clean_resid_pre = og_cache[hook.name]\n",
    "                    resid_pre[:, pos_current, :] = clean_resid_pre[:, pos_matched, :]\n",
    "                    return resid_pre\n",
    "                \n",
    "                try:\n",
    "                    # Use functools.partial to create a temporary hook function with the position fixed\n",
    "                    temp_hook_fn = partial(residual_stream_patching_hook, pos_matched=pos_matched, pos_current=pos_current)\n",
    "                    # Run the model with the patching hook\n",
    "                    patched_logits = model.run_with_hooks(prompt_tokens, fwd_hooks=[\n",
    "                        (utils.get_act_name(\"resid_pre\", skip_up_to), temp_hook_fn)\n",
    "                    ])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # def v_patching_hook(\n",
    "                #     resid_pre: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "                #     hook: HookPoint,\n",
    "                #     position: int\n",
    "                # ) -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "                #     # Each HookPoint has a name attribute giving the name of the hook.\n",
    "                #     clean_resid_pre = og_cache[hook.name]\n",
    "                #     resid_pre[:, -1, :, :] = clean_resid_pre[:, position, :, :]\n",
    "                #     return resid_pre\n",
    "                \n",
    "                # for layer in range(skip_up_to):\n",
    "                #     # Use functools.partial to create a temporary hook function with the position fixed\n",
    "                #     temp_hook_fn = partial(v_patching_hook, position=t_matched)\n",
    "                #     # Run the model with the patching hook\n",
    "                #     patched_logits = model.run_with_hooks(prompt_tokens, fwd_hooks=[\n",
    "                #         (utils.get_act_name(\"v\", layer), temp_hook_fn)\n",
    "                #     ])\n",
    "                \n",
    "                pt_topk_indices = get_top_k(patched_logits, k)\n",
    "                # get the highest prob token\n",
    "                pt_next_token = torch.tensor([pt_topk_indices[0]]).unsqueeze(0).to(og_logits.device)\n",
    "\n",
    "                # check if model can solve the task\n",
    "                decoded_pt_next_token = model.to_string(pt_next_token)[0]\n",
    "                if 'are' in decoded_pt_next_token and correct_tobe == 'are':\n",
    "                    total_solvable_pt += 1\n",
    "                elif 'is' in decoded_pt_next_token and correct_tobe == 'is':\n",
    "                    total_solvable_pt += 1\n",
    "\n",
    "                if torch.equal(og_next_token, pt_next_token):\n",
    "                    total_matches.append(1)\n",
    "                else:\n",
    "                    total_matches.append(0)\n",
    "\n",
    "                # # append the token to the sequence\n",
    "                # pt_prompt_tokens = torch.cat([prompt_tokens, pt_next_token], dim=1)\n",
    "                # # deocde the token\n",
    "                # pt_prompt = model.to_string(pt_prompt_tokens)[0]      \n",
    "                # print('PT:\\n')\n",
    "                # print(pt_prompt)\n",
    "\n",
    "                dict_pred_info[step_i]['original'] = og_topk_indices\n",
    "                dict_pred_info[step_i]['copy'] = get_top_k(patched_logits, k)\n",
    "\n",
    "            # # append the token to the sequence\n",
    "            # prompt_tokens = torch.cat([prompt_tokens, og_next_token], dim=1)\n",
    "            # # deocde the token\n",
    "            # prompt = model.to_string(prompt_tokens)[0]\n",
    "            # print('OG:\\n')\n",
    "            # print(prompt)\n",
    "            # print(\"Avg accuracy: \", total_solvable_pt / total_solvable_og)\n",
    "            # a\n",
    "\n",
    "        info_lst.append(dict_pred_info)\n",
    "        \n",
    "    jcc, acc = get_acc(info_lst)\n",
    "    print(\"Avg jaccard similarity: \", jcc)\n",
    "    print(\"Avg accuracy 2: \", acc)\n",
    "    print(\"Avg accuracy 3: \", sum(total_matches) / len(total_matches))\n",
    "\n",
    "    # print(\"Number of matched tokens: \", num_matched)\n",
    "    # print(\"Total steps: \", total_steps)\n",
    "    # print(\"%matched tokens per program: \", num_matched/total_steps)\n",
    "    # print(\"Avg matched tokens per program: \", num_matched/len(base_sents))\n",
    "    # print(\"Avg jaccard similarity: \", jcc)\n",
    "    # print(\"Avg accuracy: \", acc)\n",
    "    # print(\"Total failed programs: \", total_failed)\n",
    "\n",
    "    # return info_lst, failed_lst, num_matched, jcc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "num_samples = 2000\n",
    "subset = load_from_disk(\"/home/longnhat/workspace_phu/CopyMech/english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:num_samples]\n",
    "phrases = subset['train']['phrase'][:num_samples]\n",
    "edited_sents = subset['train']['edited_sentence'][:num_samples]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram:  5\n",
      "Skip layers:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1099/2000 [00:15<00:12, 72.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg jaccard similarity:  0.9261927847149858\n",
      "Avg accuracy 2:  0.9797979797979798\n",
      "Avg accuracy 3:  0.9797979797979798\n",
      "-----------------------------------\n",
      "n-gram:  5\n",
      "Skip layers:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1099/2000 [00:15<00:12, 71.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg jaccard similarity:  0.7707593134937785\n",
      "Avg accuracy 2:  0.8585858585858586\n",
      "Avg accuracy 3:  0.8585858585858586\n",
      "-----------------------------------\n",
      "n-gram:  5\n",
      "Skip layers:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1099/2000 [00:15<00:12, 71.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg jaccard similarity:  0.7249863002523894\n",
      "Avg accuracy 2:  0.8080808080808081\n",
      "Avg accuracy 3:  0.8080808080808081\n",
      "-----------------------------------\n",
      "n-gram:  5\n",
      "Skip layers:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1099/2000 [00:15<00:12, 71.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg jaccard similarity:  0.5956793022309764\n",
      "Avg accuracy 2:  0.5959595959595959\n",
      "Avg accuracy 3:  0.5959595959595959\n",
      "-----------------------------------\n",
      "n-gram:  5\n",
      "Skip layers:  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1099/2000 [00:15<00:12, 71.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg jaccard similarity:  0.4352043902043523\n",
      "Avg accuracy 2:  0.8282828282828283\n",
      "Avg accuracy 3:  0.8282828282828283\n",
      "-----------------------------------\n",
      "n-gram:  5\n",
      "Skip layers:  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1099/2000 [00:15<00:12, 70.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg jaccard similarity:  0.3640622705801158\n",
      "Avg accuracy 2:  0.7676767676767676\n",
      "Avg accuracy 3:  0.7676767676767676\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "extra_steps = 5\n",
    "max_steps = 1024\n",
    "k=100\n",
    "seed_everything(seed)\n",
    "\n",
    "ns = [5]\n",
    "skip_up_to = [5, 10, 15, 20, 25, 30]\n",
    "info_lst = {}\n",
    "for n in ns:\n",
    "    info_lst[n] = {}\n",
    "    for skip in skip_up_to:\n",
    "        info_lst[n][skip] = {}\n",
    "        outputs = ngram(n, model, skip, max_steps, extra_steps, k, edited_sents)\n",
    "        # info_lst[n][skip]['info'] = outputs[0]\n",
    "        # info_lst[n][skip]['failed'] = outputs[1]\n",
    "        # info_lst[n][skip]['num_matched'] = outputs[2]\n",
    "        # info_lst[n][skip]['jcc'] = outputs[3]\n",
    "        # info_lst[n][skip]['acc'] = outputs[4]\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
