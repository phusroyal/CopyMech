{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 48      # e.g. GPT-2 XL\n",
    "    n_head: int = 25\n",
    "    n_embd: int = 1600\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "            .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A GPT-like model that only stores the hidden state *after*\n",
    "    'skip_up_to - 1' layers (the \"Block k\" state).\n",
    "\n",
    "    For skipping:\n",
    "      - if we detect a copy scenario, we load the cached partial\n",
    "        hidden state from t_matched, run the last layers only.\n",
    "      - else, we run all layers and store the partial state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, skip_up_to=43):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.skip_up_to = skip_up_to   # number of layers to skip\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte':  nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe':  nn.Embedding(config.block_size, config.n_embd),\n",
    "            'h':    nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # cache_partial: only store the hidden state after skip_up_to - 1 layers\n",
    "        # so cache_partial[(batch_idx, pos)] = Tensor shape (B, T, n_embd)\n",
    "        self.cache_partial = {}\n",
    "\n",
    "    # def forward(self, input_ids, pos_matched=None):\n",
    "    #     \"\"\"\n",
    "    #     input_ids: (B, T), for simplicity assume B=1\n",
    "    #     pos_matched: int or None.\n",
    "    #       If None => normal forward on entire sequence\n",
    "    #       If not None => \"copy\" scenario.\n",
    "    #     \"\"\"\n",
    "    #     B, T = input_ids.shape\n",
    "    #     device = input_ids.device\n",
    "\n",
    "    #     # Basic embed\n",
    "    #     pos = torch.arange(0, T, device=device)\n",
    "    #     full_x = self.transformer['wte'](input_ids) + self.transformer['wpe'](pos)\n",
    "\n",
    "    #     if pos_matched is None:\n",
    "    #         # Normal forward for all T tokens from layer 0..n_layer\n",
    "    #         x = full_x\n",
    "    #         for block in self.transformer['h']:\n",
    "    #             x = block(x)\n",
    "\n",
    "    #     else:\n",
    "    #         # \"Copy\" scenario\n",
    "    #         # 1) separate the sequence into first T-1 tokens vs the newly added token\n",
    "    #         x_trunc = full_x[:, :-1, :]  # shape = (1, T-1, n_embd)\n",
    "\n",
    "    #         # 2) compute from layer 0..skip_up_to on the truncated x\n",
    "    #         for layer_idx in range(self.skip_up_to + 1):\n",
    "    #             x_trunc = self.transformer['h'][layer_idx](x_trunc)\n",
    "\n",
    "    #         # 3) get matched hidden from x_trunc for the new token\n",
    "    #         #    note that pos_matched must be < T-1, so we can do x_trunc[:, pos_matched, :]\n",
    "    #         matched_hid = x_trunc[:, pos_matched, :]  # shape (1, n_embd)\n",
    "    #         matched_hid = matched_hid.unsqueeze(1)    # => shape (1, 1, n_embd)\n",
    "\n",
    "    #         # 4) cat matched hidden to x_trunc => new shape (1, T, n_embd)\n",
    "    #         x = torch.cat([x_trunc, matched_hid], dim=1)\n",
    "\n",
    "    #         # 5) continue from layer skip_up_to+1..end on the full (1, T, n_embd)\n",
    "    #         for layer_idx in range(self.skip_up_to + 1, self.config.n_layer):\n",
    "    #             x = self.transformer['h'][layer_idx](x)\n",
    "\n",
    "    #     # final layer norm + logits\n",
    "    #     x = self.transformer['ln_f'](x)\n",
    "    #     logits = self.lm_head(x)  # shape (B, T, vocab_size)\n",
    "    #     return logits\n",
    "\n",
    "    def forward(self, input_ids, batch_idx=0, pos_new=None, pos_matched=None):\n",
    "        \"\"\"\n",
    "        input_ids: shape (B, T)\n",
    "        pos_new: which position we're generating if single-token stepping\n",
    "        pos_matched: if copying from matched position\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # token + pos embedding\n",
    "        positions = torch.arange(0, T, device=device)\n",
    "        x = self.transformer['wte'](input_ids) + self.transformer['wpe'](positions)\n",
    "\n",
    "        # Skip scenario?\n",
    "        if pos_matched is not None and (batch_idx, pos_matched) in self.cache_partial:\n",
    "            # Reuse the partial hidden state from pos_matched\n",
    "            # instead of computing layers [0..skip_up_to-1].\n",
    "            partial_hid = self.cache_partial[(batch_idx, pos_matched)].to(device)\n",
    "            x = partial_hid\n",
    "            start_layer = self.skip_up_to\n",
    "        else:\n",
    "            # we have to compute all layers or up to skip_up_to\n",
    "            start_layer = 0\n",
    "\n",
    "        # Actually run the layers from start_layer to end\n",
    "        for layer_idx in range(start_layer, self.config.n_layer):\n",
    "            block = self.transformer['h'][layer_idx]\n",
    "            x = block(x)\n",
    "\n",
    "            # If layer_idx == skip_up_to-1, store partial\n",
    "            if layer_idx == self.skip_up_to - 1 and pos_new is not None:\n",
    "                # store in CPU to reduce GPU memory\n",
    "                self.cache_partial[(batch_idx, pos_new)] = x.detach().cpu()\n",
    "\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, skip_up_to=43):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config, skip_up_to=skip_up_to)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-xl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
       "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "num_layer_cal = 10\n",
    "skip_up_to = 47-num_layer_cal\n",
    "\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_copy = GPT.from_pretrained(model_name, skip_up_to=0)\n",
    "model_copy = model_copy.to(device1)\n",
    "model_copy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following code is incorrect:\n",
      "def breadth_first_search(startnode, goalnode):\n",
      "    queue = Queue()\n",
      "    queue.append(startnode)\n",
      "\n",
      "    nodesseen = set()\n",
      "    nodesseen.add(startnode)\n",
      "\n",
      "    while True:\n",
      "        node = queue.popleft()\n",
      "\n",
      "        if node is goalnode:\n",
      "            return True\n",
      "        else:\n",
      "            queue.extend(node for node in node.successors if node not in nodesseen)\n",
      "            nodesseen.update(node.successors)\n",
      "\n",
      "    return False\n",
      "Corrected code:\n",
      "def breadth_first_search(startnode, goalnode):\n",
      "    queue = Queue()\n",
      "    queue.append(startnode)\n",
      "     nodesseen = set()\n",
      "     nodesseen.add(startnode)\n",
      "     while True:\n",
      "        node = queue.popleft()\n",
      "          if node is goalnode:\n",
      "             return True\n",
      "         else:\n",
      "           queue.extend(node for node in node.successors if node not in nodesseen)\n",
      "            nodesseen.update(node.successors)\n",
      "The above code is correct, but it is not the most efficient. The code is more\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"def breadth_first_search(startnode, goalnode):\n",
    "    queue = Queue()\n",
    "    queue.append(startnode)\n",
    "\n",
    "    nodesseen = set()\n",
    "    nodesseen.add(startnode)\n",
    "\n",
    "    while True:\n",
    "        node = queue.popleft()\n",
    "\n",
    "        if node is goalnode:\n",
    "            return True\n",
    "        else:\n",
    "            queue.extend(node for node in node.successors if node not in nodesseen)\n",
    "            nodesseen.update(node.successors)\n",
    "\n",
    "    return False\"\"\"\n",
    "\n",
    "prompt = f\"Given the following code is incorrect:\\n{code}\\nCorrected code:\"\n",
    "code_ids = tokenizer.encode(code, return_tensors='pt')\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device1)\n",
    "\n",
    "# generate 50 tokens and then decode\n",
    "steps = 20 + code_ids.size(1)\n",
    "for step in range(steps):\n",
    "    logits = model_copy(input_ids)\n",
    "    next_token = torch.argmax(logits[0, -1, :])\n",
    "    input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "print(tokenizer.decode(input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2359, -0.3648, -4.0851,  ..., -9.3595, -4.7799, -0.3083]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.2318e-05, 6.7554e-06, 1.6366e-07,  ..., 8.3809e-10, 8.1692e-08,\n",
       "          7.1478e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0430, 0.0409, 0.0212, 0.0182, 0.0171]], device='cuda:0',\n",
       "        grad_fn=<TopkBackward0>),\n",
       " tensor([[  517,   407,   845,    25, 30904]], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "top_vals, top_indices = probs.topk(5, dim=-1)\n",
    "probs, top_vals, top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[517, 407, 845, 25, 30904]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_top_k(logits, top_k=5):\n",
    "    \"\"\"\n",
    "    logits: (B, T, vocab_size)\n",
    "    Returns a list of top-k token IDs for the last position, e.g. [id1, id2,...].\n",
    "    \"\"\"\n",
    "    last_logits = logits[:, -1, :]       # shape (B, vocab_size)\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    top_vals, top_indices = probs.topk(top_k, dim=-1)\n",
    "    # top_indices is shape (B, top_k). For B=1, we do top_indices[0].tolist().\n",
    "    return top_indices[0].tolist()\n",
    "\n",
    "def detect_ngram_copy(seq_ids: torch.Tensor, n=3, skip_up_to=43):\n",
    "    \"\"\"\n",
    "    Minimal function that tries to find n-gram copy scenario\n",
    "    (just a placeholder – adapt to your real logic)\n",
    "    \"\"\"\n",
    "    T = seq_ids.size(1)  # shape (B=1, T)\n",
    "    if T < n:\n",
    "        return None, None\n",
    "    # 1) last token\n",
    "    last_token = seq_ids[0, -1].item()\n",
    "    # 2) find earlier positions of last_token\n",
    "    possible_pos = (seq_ids[0, :-1] == last_token).nonzero().view(-1)\n",
    "    if possible_pos.numel() == 0:\n",
    "        return None, None\n",
    "    # 3) check (n-1) context\n",
    "    n_minus_1 = n - 1\n",
    "    context_needed = seq_ids[0, -(n_minus_1+1):-1]  # last n-1 tokens\n",
    "    matched_pos = None\n",
    "    for pos in reversed(possible_pos):\n",
    "        if pos >= n_minus_1:\n",
    "            candidate = seq_ids[0, pos-n_minus_1:pos]\n",
    "            if torch.all(candidate == context_needed):\n",
    "                matched_pos = pos.item()\n",
    "                break\n",
    "    if matched_pos is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_pos, skip_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-xl\n",
      "loading weights from pretrained gpt: gpt2-xl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
       "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "num_layer_cal = 10\n",
    "skip_up_to = 47-num_layer_cal\n",
    "\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_copy = GPT.from_pretrained(model_name, skip_up_to=skip_up_to)\n",
    "model_copy = model_copy.to(device1)\n",
    "model_copy.eval()\n",
    "\n",
    "device2 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_original = GPT.from_pretrained(model_name, skip_up_to=0)\n",
    "model_original = model_original.to(device2)\n",
    "model_original.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "subset = load_from_disk(\"english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:1000]\n",
    "phrases = subset['train']['phrase'][:1000]\n",
    "edited_sents = subset['train']['edited_sentence'][:1000]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dadaji lost his mother and took to living with his maternal uncle Narayan Dhurmaji .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the meanwhile ,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [01:19<11:57, 19.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag:\n\u001b[1;32m     52\u001b[0m     t_matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# new position\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_matched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_matched\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m top_k_list \u001b[38;5;241m=\u001b[39m get_top_k(logits, top_k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     62\u001b[0m next_token_id \u001b[38;5;241m=\u001b[39m top_k_list[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[1], line 182\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, input_ids, batch_idx, pos_new, pos_matched)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If layer_idx == skip_up_to-1, store partial\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_up_to \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_new \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# store in CPU to reduce GPU memory\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_partial[(batch_idx, pos_new)] \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mln_f\u001b[39m\u001b[38;5;124m'\u001b[39m](x)\n\u001b[1;32m    185\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = 5\n",
    "seed_everything(seed)\n",
    "\n",
    "extra_steps = 20\n",
    "max_steps = 1024             \n",
    "k = 10\n",
    "n = 5\n",
    "info_lst = []\n",
    "\n",
    "\n",
    "for code in tqdm(file_contents):\n",
    "    prompt = f\"Given the following code is incorrect:\\n{code}\\nCorrected code:\"\n",
    "    code_ids = tokenizer.encode(code, return_tensors='pt')\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    steps = extra_steps + code_ids.size(1)\n",
    "    steps = min(steps, max_steps)\n",
    "\n",
    "    dict_pred_info = defaultdict(dict)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 3a) Copy-mech generation\n",
    "    # -------------------------------------\n",
    "    copy_ids = input_ids.clone().to(device1)  # shape (1, initial_length)\n",
    "    # (Optional) fill partial states for the entire existing prompt if you want \n",
    "    # to skip from tokens inside the prompt. For example:\n",
    "    for p in range(copy_ids.size(1)):\n",
    "        sub_ids = copy_ids[:, :p+1]\n",
    "        # pos_new=p, pos_matched=None => no skip\n",
    "        model_copy.forward(sub_ids, batch_idx=0, pos_new=p, pos_matched=None)\n",
    "        \n",
    "    flag = False\n",
    "    for step_i in range(steps):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # detect copy\n",
    "        t_matched, skipv = detect_ngram_copy(copy_ids, n=n, skip_up_to=43)\n",
    "        \n",
    "        # if t_matched is not None and copy_ids[0, t_matched].item() in [7783]:\n",
    "        #     flag = True\n",
    "        #     t_matched = None\n",
    "        \n",
    "        # if flag:\n",
    "        #     t_matched = None\n",
    "\n",
    "        logits = model_copy.forward(\n",
    "            input_ids=copy_ids,\n",
    "            batch_idx=0,\n",
    "            pos_new=copy_ids.shape[1] - 1,  # new position\n",
    "            pos_matched=t_matched\n",
    "        )\n",
    "        top_k_list = get_top_k(logits, top_k=k)\n",
    "\n",
    "        next_token_id = top_k_list[0]\n",
    "        next_token_tensor = torch.tensor([[next_token_id]], device=device1)\n",
    "        copy_ids = torch.cat([copy_ids, next_token_tensor], dim=1)\n",
    "\n",
    "        elapsed_copy = time.time() - t0\n",
    "        dict_pred_info[step_i]['copy'] = top_k_list\n",
    "        dict_pred_info[step_i]['copy_time'] = elapsed_copy\n",
    "\n",
    "    # Clear partial cache for next sample\n",
    "    model_copy.cache_partial.clear()\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 3b) Original model generation (skip_up_to=0)\n",
    "    # -------------------------------------\n",
    "    original_ids = input_ids.clone().to(device2)\n",
    "    # likewise fill partial states for the entire prompt \n",
    "    for p in range(original_ids.size(1)):\n",
    "        sub_ids = original_ids[:, :p+1]\n",
    "        model_original.forward(sub_ids, batch_idx=0, pos_new=p, pos_matched=None)\n",
    "\n",
    "    for step_i in range(steps):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # always do full pass (pos_matched=None)\n",
    "        logits = model_original.forward(\n",
    "            input_ids=original_ids,\n",
    "            batch_idx=0,\n",
    "            pos_new=original_ids.shape[1] - 1,\n",
    "            pos_matched=None\n",
    "        )\n",
    "        top_k_list = get_top_k(logits, top_k=k)\n",
    "\n",
    "        next_token_id = top_k_list[0]\n",
    "        next_token_tensor = torch.tensor([[next_token_id]], device=device2)\n",
    "        original_ids = torch.cat([original_ids, next_token_tensor], dim=1)\n",
    "\n",
    "        elapsed_orig = time.time() - t0\n",
    "        dict_pred_info[step_i]['original'] = top_k_list\n",
    "        dict_pred_info[step_i]['original_time'] = elapsed_orig\n",
    "\n",
    "    # Clear partial cache\n",
    "    model_original.cache_partial.clear()\n",
    "\n",
    "    info_lst.append(dict_pred_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return  intersection / union\n",
    "\n",
    "jcc_ult = []\n",
    "acc_ult = []\n",
    "tpt_copy = []\n",
    "tpt_orig = []\n",
    "\n",
    "for data in info_lst:\n",
    "    total_copy_time = 0\n",
    "    total_orig_time = 0\n",
    "    acc_lst = []\n",
    "    jc_lst = []\n",
    "    for step in data.keys():\n",
    "        total_copy_time += data[step]['copy_time']\n",
    "        total_orig_time += data[step]['original_time']\n",
    "        copy = data[step]['copy']\n",
    "        original = data[step]['original']\n",
    "\n",
    "        jaccard_score = jaccard_similarity(copy, original)\n",
    "        jc_lst.append(jaccard_score)\n",
    "\n",
    "        acc_score = 1 if copy[0] == original[0] else 0\n",
    "        acc_lst.append(acc_score)\n",
    "\n",
    "    # time per token - tpt\n",
    "    tpt_copy.append(total_copy_time / steps)\n",
    "    tpt_orig.append(total_orig_time / steps)\n",
    "\n",
    "    jcc_ult.append(jc_lst)\n",
    "    acc_ult.append(acc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpt_copy / tpt_orig by row \n",
    "tpt_ratio = [tpt_copy[i] / tpt_orig[i] for i in range(len(tpt_copy))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_ult[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754166875720513"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_tpt_copy = sum(tpt_copy)\n",
    "sum_tpt_orig = sum(tpt_orig)\n",
    "sum_tpt_copy / sum_tpt_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34435988335702306, 0.36635356332886493)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_avg(lsts):\n",
    "    avg_lst = []\n",
    "    for lst in lsts:\n",
    "        avg_lst.append(sum(lst) / len(lst))\n",
    "    return sum(avg_lst) / len(avg_lst)\n",
    "\n",
    "avg_jcc = cal_avg(jcc_ult)\n",
    "avg_acc = cal_avg(acc_ult)\n",
    "avg_jcc, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_ult[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "subset = load_from_disk(\"english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:1000]\n",
    "phrases = subset['train']['phrase'][:1000]\n",
    "edited_sents = subset['train']['edited_sentence'][:1000]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
