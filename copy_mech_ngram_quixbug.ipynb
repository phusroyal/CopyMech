{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 48      # e.g. GPT-2 XL\n",
    "    n_head: int = 25\n",
    "    n_embd: int = 1600\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "            .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A GPT-like model that only stores the hidden state *after*\n",
    "    'skip_up_to - 1' layers (the \"Block k\" state).\n",
    "\n",
    "    For skipping:\n",
    "      - if we detect a copy scenario, we load the cached partial\n",
    "        hidden state from t_matched, run the last layers only.\n",
    "      - else, we run all layers and store the partial state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, skip_up_to=43):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.skip_up_to = skip_up_to   # number of layers to skip\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # cache_partial: only store the hidden state after skip_up_to - 1 layers\n",
    "        # so cache_partial[(batch_idx, pos)] = Tensor shape (B, T, n_embd)\n",
    "        self.cache_partial = {}\n",
    "\n",
    "    def forward(self, input_ids, pos_matched=None):\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # Basic embed\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        full_x = self.transformer.wte(input_ids) + self.transformer.wpe(pos)\n",
    "\n",
    "        if pos_matched is None:\n",
    "            # Normal forward for all T tokens from layer 0..n_layer\n",
    "            x = full_x\n",
    "            for block in self.transformer.h:\n",
    "                x = block(x)\n",
    "\n",
    "        else:\n",
    "            # \"Copy\" scenario\n",
    "            # 1) separate the sequence into first T-1 tokens vs the newly added token\n",
    "            x_trunc = full_x[:, :-1, :]  # shape = (1, T-1, n_embd)\n",
    "\n",
    "            # 2) compute from layer 0..skip_up_to on the truncated x\n",
    "            for layer_idx in range(self.skip_up_to):\n",
    "                x_trunc = self.transformer.h[layer_idx](x_trunc)\n",
    "\n",
    "            # 3) get matched hidden from x_trunc for the new token\n",
    "            #    note that pos_matched must be < T-1, so we can do x_trunc[:, pos_matched, :]\n",
    "            matched_hid = x_trunc[:, pos_matched, :] # shape (1, n_embd)            \n",
    "            matched_hid = matched_hid.unsqueeze(1) # => shape (1, 1, n_embd)\n",
    "\n",
    "            # 4) cat matched hidden to x_trunc => new shape (1, T, n_embd)\n",
    "            x = torch.cat([x_trunc, matched_hid], dim=1)\n",
    "\n",
    "            # 5) continue from layer skip_up_to+1..end on the full (1, T, n_embd)\n",
    "            for layer_idx in range(self.skip_up_to, self.config.n_layer):\n",
    "                x = self.transformer.h[layer_idx](x)\n",
    "\n",
    "        # final layer norm + logits\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  # shape (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, skip_up_to=43):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config, skip_up_to=skip_up_to)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_top_k(logits, top_k=5):\n",
    "    \"\"\"\n",
    "    logits: (B, T, vocab_size)\n",
    "    Returns a list of top-k token IDs for the last position, e.g. [id1, id2,...].\n",
    "    \"\"\"\n",
    "    last_logits = logits[:, -1, :]       # shape (B, vocab_size)\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    top_vals, top_indices = probs.topk(top_k, dim=-1)\n",
    "    # top_indices is shape (B, top_k). For B=1, we do top_indices[0].tolist().\n",
    "    return top_indices[0].tolist()\n",
    "\n",
    "def detect_ngram_copy(seq_ids: torch.Tensor, n=3, skip_up_to=43):\n",
    "    \"\"\"\n",
    "    Minimal function that tries to find n-gram copy scenario\n",
    "    (just a placeholder – adapt to your real logic)\n",
    "    \"\"\"\n",
    "    T = seq_ids.size(1)  # shape (B=1, T)\n",
    "    if T < n:\n",
    "        return None, None\n",
    "    # 1) last token\n",
    "    last_token = seq_ids[0, -1].item()\n",
    "    # 2) find earlier positions of last_token\n",
    "    possible_pos = (seq_ids[0, :-1] == last_token).nonzero().view(-1)\n",
    "    if possible_pos.numel() == 0:\n",
    "        return None, None\n",
    "    # 3) check (n-1) context\n",
    "    n_minus_1 = n - 1\n",
    "    context_needed = seq_ids[0, -(n_minus_1+1):-1]  # last n-1 tokens\n",
    "    matched_pos = None\n",
    "    for pos in reversed(possible_pos):\n",
    "        if pos >= n_minus_1:\n",
    "            candidate = seq_ids[0, pos-n_minus_1:pos]\n",
    "            if torch.all(candidate == context_needed):\n",
    "                matched_pos = pos.item()\n",
    "                break\n",
    "    if matched_pos is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_pos, skip_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return  intersection / union\n",
    "\n",
    "\n",
    "def get_acc(info_lst):\n",
    "    jcc_ult = []\n",
    "    acc_ult = []\n",
    "\n",
    "    for data in info_lst:\n",
    "        acc_lst = []\n",
    "        jc_lst = []\n",
    "        for step in data.keys():\n",
    "            copy = data[step]['copy']\n",
    "            original = data[step]['original']\n",
    "\n",
    "            jaccard_score = jaccard_similarity(copy, original)\n",
    "            jc_lst.append(jaccard_score)\n",
    "\n",
    "            acc_score = 1 if copy[0] == original[0] else 0\n",
    "            acc_lst.append(acc_score)\n",
    "\n",
    "        jcc_ult.append(jc_lst)\n",
    "        acc_ult.append(acc_lst)\n",
    "    \n",
    "    def cal_avg(lsts):\n",
    "        avg_lst = []\n",
    "        for lst in lsts:\n",
    "            avg_lst.append(sum(lst) / len(lst))\n",
    "        return sum(avg_lst) / len(avg_lst)\n",
    "\n",
    "    avg_jcc = cal_avg(jcc_ult)\n",
    "    avg_acc = cal_avg(acc_ult)\n",
    "    print(\"Average Jaccard Similarity: \", avg_jcc)\n",
    "    print(\"Average Accuracy: \", avg_acc)\n",
    "\n",
    "    return avg_jcc, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "skip_up_to = 5\n",
    "\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_copy = GPT.from_pretrained(model_name, skip_up_to=skip_up_to)\n",
    "model_copy = model_copy.to(device1)\n",
    "model_copy.eval()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT.from_pretrained(model_name, skip_up_to=skip_up_to)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder containing the .py files\n",
    "folder_path = \"QuixBugs/python_programs\"  # Replace with the actual path to your folder\n",
    "\n",
    "# Initialize an empty list to store file contents\n",
    "file_contents = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".py\"):  # Check if the file is a .py file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            content = file.read()  # Read the file content\n",
    "            content = content.replace(\"    \", \"\\t\")\n",
    "            file_contents.append(content)  # Add the content to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model match with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def mini_acc(dict_pred_info):\n",
    "    acc_lst = []\n",
    "    for step in dict_pred_info.keys():\n",
    "        copy = dict_pred_info[step]['copy']\n",
    "        original = dict_pred_info[step]['original']\n",
    "\n",
    "        acc_score = 1 if copy[0] == original[0] else 0\n",
    "        acc_lst.append(acc_score)\n",
    "    return sum(acc_lst) / len(acc_lst)\n",
    "\n",
    "def ngram(n, model_copy, model, skip_up_to, max_steps, extra_steps, k, file_contents):\n",
    "    model_copy.eval()\n",
    "    model.eval()\n",
    "    model.skip_up_to = 0\n",
    "\n",
    "    print(\"n-gram: \", n)\n",
    "    print(\"Previous skip layers: \", model_copy.skip_up_to)\n",
    "    model_copy.skip_up_to = skip_up_to\n",
    "    print(\"New skip layers: \", model_copy.skip_up_to)\n",
    "\n",
    "    info_lst = []\n",
    "    failed_lst = []\n",
    "    total_failed = 0\n",
    "\n",
    "    num_matched = 0\n",
    "    for code in tqdm(file_contents):\n",
    "        prompt = f\"Correct the following code:\\n{code}\\nCorrected code: def\"\n",
    "        \n",
    "        code_ids = tokenizer.encode(code, return_tensors='pt')\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        steps = extra_steps+code_ids.size(1)\n",
    "        if steps > max_steps:\n",
    "            steps = max_steps\n",
    "\n",
    "        dict_pred_info = defaultdict(dict)\n",
    "\n",
    "        # Copy model generation\n",
    "        copy_ids = input_ids.clone().to(device1)\n",
    "        for step_i in range(steps):\n",
    "            t0 = time.time()\n",
    "\n",
    "            # detect copy scenario\n",
    "            t_matched, skip_up_to = detect_ngram_copy(copy_ids, n=n, skip_up_to=skip_up_to)\n",
    "            if t_matched is not None:\n",
    "                num_matched += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass (copy-mech model)\n",
    "                logits = model_copy(\n",
    "                    input_ids=copy_ids,\n",
    "                    pos_matched=t_matched,\n",
    "                )\n",
    "\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, k, dim=-1)                \n",
    "                # get the highest prob token\n",
    "                next_token = topk_indices[:, 0].unsqueeze(1)\n",
    "                # append the token to the sequence\n",
    "                copy_ids = torch.cat([copy_ids, next_token], dim=1)\n",
    "\n",
    "            elapsed_copy = time.time() - t0\n",
    "\n",
    "            # store info\n",
    "            dict_pred_info[step_i]['copy'] = topk_indices[0].tolist()\n",
    "            dict_pred_info[step_i]['copy_time'] = elapsed_copy\n",
    "        model_copy.cache_partial.clear()\n",
    "        \n",
    "        # 2) Original model generation\n",
    "        t_matched, skip_up_to = (None, None)\n",
    "        original_ids = input_ids.clone().to(device)\n",
    "        for step_i in range(steps):\n",
    "            t0 = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass\n",
    "                logits = model(\n",
    "                    input_ids=original_ids,\n",
    "                )\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, k, dim=-1)\n",
    "                # get the highest prob token\n",
    "                next_token = topk_indices[:, 0].unsqueeze(1)\n",
    "                # append the token to the sequence\n",
    "                original_ids = torch.cat([original_ids, next_token], dim=1)\n",
    "                \n",
    "            elapsed_orig = time.time() - t0\n",
    "\n",
    "            # store info\n",
    "            dict_pred_info[step_i]['original'] = topk_indices[0].tolist()\n",
    "            dict_pred_info[step_i]['original_time'] = elapsed_orig\n",
    "\n",
    "        model.cache_partial.clear()\n",
    "        current_acc = mini_acc(dict_pred_info)\n",
    "        \n",
    "        if current_acc < 1:\n",
    "            print(\"Failed accuracy: \", current_acc)\n",
    "            failed = {}\n",
    "            failed['code'] = code\n",
    "            failed['predicted'] = tokenizer.decode(copy_ids[0])\n",
    "            failed['original'] = tokenizer.decode(original_ids[0])\n",
    "            failed['info'] = dict_pred_info\n",
    "            failed['acc'] = current_acc\n",
    "            failed_lst.append(failed)    \n",
    "            total_failed += 1\n",
    "\n",
    "        info_lst.append(dict_pred_info)\n",
    "\n",
    "    print(\"Number of matched tokens: \", num_matched)\n",
    "    print(\"Avg matched tokens per program: \", num_matched/len(file_contents))\n",
    "    print(\"Total failed programs: \", total_failed)\n",
    "    jcc, acc = get_acc(info_lst)\n",
    "    return info_lst, failed_lst, num_matched, jcc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram:  5\n",
      "Previous skip layers:  5\n",
      "New skip layers:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:28<18:28, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.06467661691542288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:43<13:06, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.2620689655172414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [02:02<08:09, 15.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.03896103896103896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [02:35<07:54, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.9440559440559441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [03:10<08:23, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.44976076555023925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [03:31<04:37, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.07964601769911504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [04:31<06:14, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.0859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [05:13<06:45, 20.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.04072398190045249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [05:38<06:52, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.16129032258064516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [05:55<06:05, 20.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.1118421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [06:21<06:15, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.08376963350785341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [06:57<04:55, 19.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.1388888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [07:21<04:55, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.34972677595628415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [08:16<03:01, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.2535885167464115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [08:30<02:32, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.20714285714285716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [09:19<02:16, 19.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.059322033898305086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [09:43<02:05, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.56353591160221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [09:52<01:26, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.34579439252336447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [10:06<01:04, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:36<00:00, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched tokens:  3241\n",
      "Avg matched tokens per program:  81.025\n",
      "Total failed programs:  19\n",
      "Average Jaccard Similarity:  0.64257039534966\n",
      "Average Accuracy:  0.6435183067177357\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "extra_steps = 30\n",
    "max_steps = 1024\n",
    "k=100\n",
    "seed_everything(seed)\n",
    "\n",
    "ns = [5]\n",
    "skip_up_to = [5]\n",
    "info_lst = {}\n",
    "for n in ns:\n",
    "    info_lst[n] = {}\n",
    "    for skip in skip_up_to:\n",
    "        info_lst[n][skip] = {}\n",
    "        outputs = ngram(n, model_copy, model, skip, max_steps, extra_steps, k, file_contents)\n",
    "        info_lst[n][skip]['info'] = outputs[0]\n",
    "        info_lst[n][skip]['failed'] = outputs[1]\n",
    "        info_lst[n][skip]['num_matched'] = outputs[2]\n",
    "        info_lst[n][skip]['jcc'] = outputs[3]\n",
    "        info_lst[n][skip]['acc'] = outputs[4]\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "subset = load_from_disk(\"english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:1000]\n",
    "phrases = subset['train']['phrase'][:1000]\n",
    "edited_sents = subset['train']['edited_sentence'][:1000]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
