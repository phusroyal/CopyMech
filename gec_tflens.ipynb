{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_top_k(logits, top_k=5):\n",
    "    \"\"\"\n",
    "    logits: (B, T, vocab_size)\n",
    "    Returns a list of top-k token IDs for the last position, e.g. [id1, id2,...].\n",
    "    \"\"\"\n",
    "    last_logits = logits[:, -1, :]       # shape (B, vocab_size)\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    top_vals, top_indices = probs.topk(top_k, dim=-1)\n",
    "    # top_indices is shape (B, top_k). For B=1, we do top_indices[0].tolist().\n",
    "    return top_indices[0].tolist()\n",
    "\n",
    "def detect_ngram_copy(seq_ids: torch.Tensor, n=3, skip_up_to=43):\n",
    "    \"\"\"\n",
    "    Minimal function that tries to find n-gram copy scenario\n",
    "    (just a placeholder â€“ adapt to your real logic)\n",
    "    \"\"\"\n",
    "    T = seq_ids.size(1)  # shape (B=1, T)\n",
    "    if T < n:\n",
    "        return None, None\n",
    "    # 1) last token\n",
    "    last_token = seq_ids[0, -1].item()\n",
    "    # 2) find earlier positions of last_token\n",
    "    possible_pos = (seq_ids[0, :-1] == last_token).nonzero().view(-1)\n",
    "    if possible_pos.numel() == 0:\n",
    "        return None, None\n",
    "    # 3) check (n-1) context\n",
    "    n_minus_1 = n - 1\n",
    "    context_needed = seq_ids[0, -(n_minus_1+1):-1]  # last n-1 tokens\n",
    "    matched_pos = None\n",
    "    for pos in reversed(possible_pos):\n",
    "        if pos >= n_minus_1:\n",
    "            candidate = seq_ids[0, pos-n_minus_1:pos]\n",
    "            if torch.all(candidate == context_needed):\n",
    "                matched_pos = pos.item()\n",
    "                break\n",
    "    if matched_pos is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_pos, skip_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return  intersection / union\n",
    "\n",
    "\n",
    "def get_acc(info_lst):\n",
    "    jcc_ult = []\n",
    "    acc_ult = []\n",
    "\n",
    "    for data in info_lst:\n",
    "        acc_lst = []\n",
    "        jc_lst = []\n",
    "        for step in data.keys():\n",
    "            copy = data[step]['copy']\n",
    "            original = data[step]['original']\n",
    "\n",
    "            jaccard_score = jaccard_similarity(copy, original)\n",
    "            jc_lst.append(jaccard_score)\n",
    "\n",
    "            acc_score = 1 if copy[0] == original[0] else 0\n",
    "            acc_lst.append(acc_score)\n",
    "\n",
    "        jcc_ult.append(jc_lst)\n",
    "        acc_ult.append(acc_lst)\n",
    "    \n",
    "    def cal_avg(lsts):\n",
    "        avg_lst = []\n",
    "        for lst in lsts:\n",
    "            if len(lst) == 0:\n",
    "                continue\n",
    "            avg_lst.append(sum(lst) / len(lst))\n",
    "        return sum(avg_lst) / len(avg_lst)\n",
    "\n",
    "    avg_jcc = cal_avg(jcc_ult)\n",
    "    avg_acc = cal_avg(acc_ult)\n",
    "\n",
    "    return avg_jcc, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ff75c939f30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb41bc155c84313abb39bd5bac29ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-3B into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Qwen2Tokenizer'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "# load model and tokenizer\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def mini_acc(dict_pred_info):\n",
    "    acc_lst = []\n",
    "    for step in dict_pred_info.keys():\n",
    "        copy = dict_pred_info[step]['copy']\n",
    "        original = dict_pred_info[step]['original']\n",
    "\n",
    "        acc_score = 1 if copy[0] == original[0] else 0\n",
    "        acc_lst.append(acc_score)\n",
    "    return sum(acc_lst) / len(acc_lst)\n",
    "\n",
    "def has_required_spaces(seq: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the sequence has an occurrence of 'is' or 'are'\n",
    "    that is preceded (anywhere earlier in the sequence) by at least 6 tokens that are exactly 'space'.\n",
    "    \n",
    "    Examples:\n",
    "      'There space space space space space oh space is a cat.' -> True\n",
    "      'There space are many cats.' -> False\n",
    "      'There is a cat.' -> False\n",
    "      'There space space space space space space is a cat.' -> True\n",
    "      'There spaces are many cats.' -> False\n",
    "    \"\"\"\n",
    "    tokens = seq.split()\n",
    "    # check if sentence only has 1 is or are\n",
    "    if tokens.count(\"is\") + tokens.count(\"are\") != 1:\n",
    "        return False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in {\"is\", \"are\"}:\n",
    "            # Count how many tokens before this occurrence are exactly \"space\"\n",
    "            if len(tokens[:i]) >= 6:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\"Given a text, replace ' is ' by ' are ', and vice versa. Return the corrupted text, and the text until the first is/are.\"\"\"\n",
    "    text = text.strip()\n",
    "    if ' is ' in text:\n",
    "        corrupted_text = text.replace(' is ', ' are ', 1)\n",
    "    elif ' are ' in text:\n",
    "        corrupted_text = text.replace(' are ', ' is ', 1)\n",
    "    \n",
    "    # find position of first is/are and return text before that\n",
    "    first_is = text.find(' is ')\n",
    "    first_are = text.find(' are ')\n",
    "    if first_is == -1 and first_are == -1:\n",
    "        return None\n",
    "    elif first_is == -1:\n",
    "        return corrupted_text, text[:first_are], 'are'\n",
    "    elif first_are == -1:\n",
    "        return corrupted_text, text[:first_is], 'is'\n",
    "    \n",
    "    return corrupted_text, text[:min(first_is, first_are)]\n",
    "\n",
    "def ngram(n, model, skip_up_to, max_steps, extra_steps, k, edited_phrases):\n",
    "\n",
    "    print(\"n-gram: \", n)\n",
    "    print(\"Skip layers: \", skip_up_to)\n",
    "\n",
    "    info_lst = []\n",
    "    failed_lst = []\n",
    "\n",
    "    total_failed_by_prepocess = 0\n",
    "    total_failed_as_ulsolvable = 0\n",
    "    total_solvable_og = 0\n",
    "    total_solvable_pt = 0\n",
    "\n",
    "    num_matched = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    is_encoded = tokenizer.encode(\"is\", return_tensors='pt').to(model.device)\n",
    "    are_encoded = tokenizer.encode(\"are\", return_tensors='pt').to(model.device)\n",
    "\n",
    "    for edited in tqdm(edited_phrases):\n",
    "\n",
    "        if total_solvable_og == 100:\n",
    "            break\n",
    "\n",
    "        if not has_required_spaces(edited):\n",
    "            continue\n",
    "\n",
    "        # preprocess text\n",
    "        edited = text_preprocess(edited)\n",
    "        if edited is None:\n",
    "            total_failed_by_prepocess += 1\n",
    "            continue\n",
    "        corrupted_text, pre_isare, correct_tobe = edited\n",
    "        prompt = f\"Please fix grammar of the following text: '{corrupted_text}'. The correct text is: {pre_isare}\"\n",
    "\n",
    "        # edited_ids = tokenizer.encode(edited, return_tensors='pt')\n",
    "        # steps = extra_steps + edited_ids.size(1)\n",
    "        # if steps > max_steps:\n",
    "        #     steps = max_steps\n",
    "        # total_steps += steps\n",
    "        steps = 1\n",
    "\n",
    "        dict_pred_info = defaultdict(dict)\n",
    "\n",
    "        for step_i in range(steps):\n",
    "            prompt_tokens = model.to_tokens(prompt)\n",
    "\n",
    "            if step_i != 0:\n",
    "                prompt_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
    "\n",
    "            # run on the prompt once with cache to store activations to patch in later\n",
    "            og_logits, og_cache = model.run_with_cache(prompt_tokens)\n",
    "            # get the top k tokens\n",
    "            og_topk_indices = get_top_k(og_logits, k)\n",
    "            # get the highest prob token\n",
    "            og_next_token = torch.tensor([og_topk_indices[0]]).unsqueeze(0).to(og_logits.device)\n",
    "\n",
    "            # check if model is predicting is or are\n",
    "            if correct_tobe == 'is':\n",
    "                if og_next_token == are_encoded:\n",
    "                    total_failed_as_ulsolvable += 1\n",
    "                    continue\n",
    "            elif correct_tobe == 'are':\n",
    "                if og_next_token == is_encoded:\n",
    "                    total_failed_as_ulsolvable += 1\n",
    "                    continue\n",
    "            total_solvable_og += 1            \n",
    "\n",
    "            # detect copy scenario\n",
    "            t_matched, _ = detect_ngram_copy(prompt_tokens, n=n, skip_up_to=skip_up_to)\n",
    "            if t_matched is not None:\n",
    "                num_matched += 1\n",
    "\n",
    "                def residual_stream_patching_hook(\n",
    "                    resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "                    hook: HookPoint,\n",
    "                    position: int\n",
    "                ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "                    # Each HookPoint has a name attribute giving the name of the hook.\n",
    "                    clean_resid_pre = og_cache[hook.name]\n",
    "                    resid_pre[:, -1, :] = clean_resid_pre[:, position, :]\n",
    "                    return resid_pre\n",
    "                \n",
    "                # Use functools.partial to create a temporary hook function with the position fixed\n",
    "                temp_hook_fn = partial(residual_stream_patching_hook, position=t_matched)\n",
    "                # Run the model with the patching hook\n",
    "                patched_logits = model.run_with_hooks(prompt_tokens, fwd_hooks=[\n",
    "                    (utils.get_act_name(\"resid_pre\", skip_up_to), temp_hook_fn)\n",
    "                ])\n",
    "\n",
    "                # def v_patching_hook(\n",
    "                #     resid_pre: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "                #     hook: HookPoint,\n",
    "                #     position: int\n",
    "                # ) -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "                #     # Each HookPoint has a name attribute giving the name of the hook.\n",
    "                #     clean_resid_pre = og_cache[hook.name]\n",
    "                #     resid_pre[:, -1, :, :] = clean_resid_pre[:, position, :, :]\n",
    "                #     return resid_pre\n",
    "                \n",
    "                # for layer in range(skip_up_to):\n",
    "                #     # Use functools.partial to create a temporary hook function with the position fixed\n",
    "                #     temp_hook_fn = partial(v_patching_hook, position=t_matched)\n",
    "                #     # Run the model with the patching hook\n",
    "                #     patched_logits = model.run_with_hooks(prompt_tokens, fwd_hooks=[\n",
    "                #         (utils.get_act_name(\"v\", layer), temp_hook_fn)\n",
    "                #     ])\n",
    "                \n",
    "                pt_topk_indices = get_top_k(og_logits, k)\n",
    "                # get the highest prob token\n",
    "                pt_next_token = torch.tensor([pt_topk_indices[0]]).unsqueeze(0).to(og_logits.device)\n",
    "\n",
    "                # check if model is predicting is or are\n",
    "                if correct_tobe == 'is':\n",
    "                    if pt_next_token == are_encoded:\n",
    "                        total_failed_as_ulsolvable += 1\n",
    "                        continue\n",
    "                elif correct_tobe == 'are':\n",
    "                    if og_next_token == is_encoded:\n",
    "                        total_failed_as_ulsolvable += 1\n",
    "                        continue\n",
    "                total_solvable_pt += 1      \n",
    "\n",
    "                dict_pred_info[step_i]['original'] = og_topk_indices\n",
    "                dict_pred_info[step_i]['copy'] = get_top_k(patched_logits, k)\n",
    "\n",
    "            # append the token to the sequence\n",
    "            prompt_tokens = torch.cat([prompt_tokens, og_next_token], dim=1)\n",
    "            # deocde the token\n",
    "            prompt = model.to_string(prompt_tokens)[0]\n",
    "            print(prompt)\n",
    "            a\n",
    "\n",
    "        info_lst.append(dict_pred_info)\n",
    "\n",
    "    \n",
    "    print(\"Avg accuracy: \", total_solvable_pt / total_solvable_og)\n",
    "        \n",
    "    # jcc, acc = get_acc(info_lst)\n",
    "    # print(\"Number of matched tokens: \", num_matched)\n",
    "    # print(\"Total steps: \", total_steps)\n",
    "    # print(\"%matched tokens per program: \", num_matched/total_steps)\n",
    "    # print(\"Avg matched tokens per program: \", num_matched/len(base_sents))\n",
    "    # print(\"Avg jaccard similarity: \", jcc)\n",
    "    # print(\"Avg accuracy: \", acc)\n",
    "    # print(\"Total failed programs: \", total_failed)\n",
    "\n",
    "    # return info_lst, failed_lst, num_matched, jcc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "num_samples = 1000\n",
    "subset = load_from_disk(\"/home/longnhat/workspace_phu/CopyMech/english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:num_samples]\n",
    "phrases = subset['train']['phrase'][:num_samples]\n",
    "edited_sents = subset['train']['edited_sentence'][:num_samples]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram:  5\n",
      "Skip layers:  [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<02:35,  6.43it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m skip \u001b[38;5;129;01min\u001b[39;00m skip_up_to:\n\u001b[1;32m     13\u001b[0m     info_lst[n][skip] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mngram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_up_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medited_sents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     info_lst[n][skip][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     info_lst[n][skip][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 118\u001b[0m, in \u001b[0;36mngram\u001b[0;34m(n, model, skip_up_to, max_steps, extra_steps, k, edited_phrases)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# check if model is predicting is or are\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m correct_tobe \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mog_next_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mare_encoded\u001b[49m:\n\u001b[1;32m    119\u001b[0m         total_failed_as_ulsolvable \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "extra_steps = 5\n",
    "max_steps = 1024\n",
    "k=100\n",
    "seed_everything(seed)\n",
    "\n",
    "ns = [5]\n",
    "skip_up_to = [5]\n",
    "info_lst = {}\n",
    "for n in ns:\n",
    "    info_lst[n] = {}\n",
    "    for skip in skip_up_to:\n",
    "        info_lst[n][skip] = {}\n",
    "        outputs = ngram(n, model, skip_up_to, max_steps, extra_steps, k, edited_sents)\n",
    "        info_lst[n][skip]['info'] = outputs[0]\n",
    "        info_lst[n][skip]['failed'] = outputs[1]\n",
    "        info_lst[n][skip]['num_matched'] = outputs[2]\n",
    "        info_lst[n][skip]['jcc'] = outputs[3]\n",
    "        info_lst[n][skip]['acc'] = outputs[4]\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
