{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 48      # e.g. GPT-2 XL\n",
    "    n_head: int = 25\n",
    "    n_embd: int = 1600\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "            .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A GPT-like model that only stores the hidden state *after*\n",
    "    'skip_up_to - 1' layers (the \"Block k\" state).\n",
    "\n",
    "    For skipping:\n",
    "      - if we detect a copy scenario, we load the cached partial\n",
    "        hidden state from t_matched, run the last layers only.\n",
    "      - else, we run all layers and store the partial state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, skip_up_to=43):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.skip_up_to = skip_up_to   # number of layers to skip\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # cache_partial: only store the hidden state after skip_up_to - 1 layers\n",
    "        # so cache_partial[(batch_idx, pos)] = Tensor shape (B, T, n_embd)\n",
    "        self.cache_partial = {}\n",
    "\n",
    "    def forward(self, input_ids, pos_matched=None):\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # Basic embed\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        full_x = self.transformer.wte(input_ids) + self.transformer.wpe(pos)\n",
    "\n",
    "        if pos_matched is None:\n",
    "            # Normal forward for all T tokens from layer 0..n_layer\n",
    "            x = full_x\n",
    "            for block in self.transformer.h:\n",
    "                x = block(x)\n",
    "\n",
    "        else:\n",
    "            # \"Copy\" scenario\n",
    "            # 1) separate the sequence into first T-1 tokens vs the newly added token\n",
    "            x_trunc = full_x[:, :-1, :]  # shape = (1, T-1, n_embd)\n",
    "\n",
    "            # 2) compute from layer 0..skip_up_to on the truncated x\n",
    "            for layer_idx in range(self.skip_up_to):\n",
    "                x_trunc = self.transformer.h[layer_idx](x_trunc)\n",
    "\n",
    "            # 3) get matched hidden from x_trunc for the new token\n",
    "            #    note that pos_matched must be < T-1, so we can do x_trunc[:, pos_matched, :]\n",
    "            matched_hid = x_trunc[:, pos_matched, :] # shape (1, n_embd)            \n",
    "            matched_hid = matched_hid.unsqueeze(1) # => shape (1, 1, n_embd)\n",
    "\n",
    "            # 4) cat matched hidden to x_trunc => new shape (1, T, n_embd)\n",
    "            x = torch.cat([x_trunc, matched_hid], dim=1)\n",
    "\n",
    "            # 5) continue from layer skip_up_to+1..end on the full (1, T, n_embd)\n",
    "            for layer_idx in range(self.skip_up_to, self.config.n_layer):\n",
    "                x = self.transformer.h[layer_idx](x)\n",
    "\n",
    "        # final layer norm + logits\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  # shape (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, skip_up_to=43):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config, skip_up_to=skip_up_to)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_top_k(logits, top_k=5):\n",
    "    \"\"\"\n",
    "    logits: (B, T, vocab_size)\n",
    "    Returns a list of top-k token IDs for the last position, e.g. [id1, id2,...].\n",
    "    \"\"\"\n",
    "    last_logits = logits[:, -1, :]       # shape (B, vocab_size)\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    top_vals, top_indices = probs.topk(top_k, dim=-1)\n",
    "    # top_indices is shape (B, top_k). For B=1, we do top_indices[0].tolist().\n",
    "    return top_indices[0].tolist()\n",
    "\n",
    "def detect_ngram_copy(seq_ids: torch.Tensor, n=3, skip_up_to=43):\n",
    "    \"\"\"\n",
    "    Minimal function that tries to find n-gram copy scenario\n",
    "    (just a placeholder â€“ adapt to your real logic)\n",
    "    \"\"\"\n",
    "    T = seq_ids.size(1)  # shape (B=1, T)\n",
    "    if T < n:\n",
    "        return None, None\n",
    "    # 1) last token\n",
    "    last_token = seq_ids[0, -1].item()\n",
    "    # 2) find earlier positions of last_token\n",
    "    possible_pos = (seq_ids[0, :-1] == last_token).nonzero().view(-1)\n",
    "    if possible_pos.numel() == 0:\n",
    "        return None, None\n",
    "    # 3) check (n-1) context\n",
    "    n_minus_1 = n - 1\n",
    "    context_needed = seq_ids[0, -(n_minus_1+1):-1]  # last n-1 tokens\n",
    "    matched_pos = None\n",
    "    for pos in reversed(possible_pos):\n",
    "        if pos >= n_minus_1:\n",
    "            candidate = seq_ids[0, pos-n_minus_1:pos]\n",
    "            if torch.all(candidate == context_needed):\n",
    "                matched_pos = pos.item()\n",
    "                break\n",
    "    if matched_pos is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_pos, skip_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return  intersection / union\n",
    "\n",
    "\n",
    "def get_acc(info_lst):\n",
    "    jcc_ult = []\n",
    "    acc_ult = []\n",
    "\n",
    "    for data in info_lst:\n",
    "        acc_lst = []\n",
    "        jc_lst = []\n",
    "        for step in data.keys():\n",
    "            copy = data[step]['copy']\n",
    "            original = data[step]['original']\n",
    "\n",
    "            jaccard_score = jaccard_similarity(copy, original)\n",
    "            jc_lst.append(jaccard_score)\n",
    "\n",
    "            acc_score = 1 if copy[0] == original[0] else 0\n",
    "            acc_lst.append(acc_score)\n",
    "\n",
    "        jcc_ult.append(jc_lst)\n",
    "        acc_ult.append(acc_lst)\n",
    "    \n",
    "    def cal_avg(lsts):\n",
    "        avg_lst = []\n",
    "        for lst in lsts:\n",
    "            avg_lst.append(sum(lst) / len(lst))\n",
    "        return sum(avg_lst) / len(avg_lst)\n",
    "\n",
    "    avg_jcc = cal_avg(jcc_ult)\n",
    "    avg_acc = cal_avg(acc_ult)\n",
    "    print(\"Average Jaccard Similarity: \", avg_jcc)\n",
    "    print(\"Average Accuracy: \", avg_acc)\n",
    "\n",
    "    return avg_jcc, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/longnhat/miniconda3/envs/icl_mi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-xl\n",
      "loading weights from pretrained gpt: gpt2-xl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
       "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "skip_up_to = 5\n",
    "\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_copy = GPT.from_pretrained(model_name, skip_up_to=skip_up_to)\n",
    "model_copy = model_copy.to(device1)\n",
    "model_copy.eval()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT.from_pretrained(model_name, skip_up_to=skip_up_to)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "subset = load_from_disk(\"english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:60]\n",
    "phrases = subset['train']['phrase'][:60]\n",
    "edited_sents = subset['train']['edited_sentence'][:60]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model match with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def mini_acc(dict_pred_info):\n",
    "    acc_lst = []\n",
    "    for step in dict_pred_info.keys():\n",
    "        copy = dict_pred_info[step]['copy']\n",
    "        original = dict_pred_info[step]['original']\n",
    "\n",
    "        acc_score = 1 if copy[0] == original[0] else 0\n",
    "        acc_lst.append(acc_score)\n",
    "    return sum(acc_lst) / len(acc_lst)\n",
    "\n",
    "def ngram(n, model_copy, model, skip_up_to, max_steps, extra_steps, k, base_sents, phrases):\n",
    "    model_copy.eval()\n",
    "    model.eval()\n",
    "    model.skip_up_to = 0\n",
    "\n",
    "    print(\"n-gram: \", n)\n",
    "    print(\"Previous skip layers: \", model_copy.skip_up_to)\n",
    "    model_copy.skip_up_to = skip_up_to\n",
    "    print(\"New skip layers: \", model_copy.skip_up_to)\n",
    "\n",
    "    info_lst = []\n",
    "    failed_lst = []\n",
    "    total_failed = 0\n",
    "\n",
    "    num_matched = 0\n",
    "    for base, phrase in tqdm(zip(base_sents, phrases)):\n",
    "        prompt = f\"Insert this phrase of '{phrase}' in to this sentence of '{base}'. The inserted sentence is:\"\n",
    "        \n",
    "        code_ids = tokenizer.encode(base, return_tensors='pt')\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        steps = extra_steps+code_ids.size(1)\n",
    "        if steps > max_steps:\n",
    "            steps = max_steps\n",
    "\n",
    "        dict_pred_info = defaultdict(dict)\n",
    "\n",
    "        # Copy model generation\n",
    "        copy_ids = input_ids.clone().to(device1)\n",
    "        for step_i in range(steps):\n",
    "            t0 = time.time()\n",
    "\n",
    "            # detect copy scenario\n",
    "            t_matched, skip_up_to = detect_ngram_copy(copy_ids, n=n, skip_up_to=skip_up_to)\n",
    "            if t_matched is not None:\n",
    "                num_matched += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass (copy-mech model)\n",
    "                logits = model_copy(\n",
    "                    input_ids=copy_ids,\n",
    "                    pos_matched=t_matched,\n",
    "                )\n",
    "\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, k, dim=-1)                \n",
    "                # get the highest prob token\n",
    "                next_token = topk_indices[:, 0].unsqueeze(1)\n",
    "                # append the token to the sequence\n",
    "                copy_ids = torch.cat([copy_ids, next_token], dim=1)\n",
    "\n",
    "            elapsed_copy = time.time() - t0\n",
    "\n",
    "            # store info\n",
    "            dict_pred_info[step_i]['copy'] = topk_indices[0].tolist()\n",
    "            dict_pred_info[step_i]['copy_time'] = elapsed_copy\n",
    "        model_copy.cache_partial.clear()\n",
    "        \n",
    "        # 2) Original model generation\n",
    "        t_matched, skip_up_to = (None, None)\n",
    "        original_ids = input_ids.clone().to(device)\n",
    "        for step_i in range(steps):\n",
    "            t0 = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass\n",
    "                logits = model(\n",
    "                    input_ids=original_ids,\n",
    "                )\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, k, dim=-1)\n",
    "                # get the highest prob token\n",
    "                next_token = topk_indices[:, 0].unsqueeze(1)\n",
    "                # append the token to the sequence\n",
    "                original_ids = torch.cat([original_ids, next_token], dim=1)\n",
    "                \n",
    "            elapsed_orig = time.time() - t0\n",
    "\n",
    "            # store info\n",
    "            dict_pred_info[step_i]['original'] = topk_indices[0].tolist()\n",
    "            dict_pred_info[step_i]['original_time'] = elapsed_orig\n",
    "\n",
    "        model.cache_partial.clear()\n",
    "        current_acc = mini_acc(dict_pred_info)\n",
    "        \n",
    "        if current_acc < 1:\n",
    "            print(\"Failed accuracy: \", current_acc)\n",
    "            failed = {}\n",
    "            failed['predicted'] = tokenizer.decode(copy_ids[0])\n",
    "            failed['original'] = tokenizer.decode(original_ids[0])\n",
    "            failed['info'] = dict_pred_info\n",
    "            failed['acc'] = current_acc\n",
    "            failed_lst.append(failed)    \n",
    "            total_failed += 1\n",
    "\n",
    "        info_lst.append(dict_pred_info)\n",
    "\n",
    "    print(\"Number of matched tokens: \", num_matched)\n",
    "    print(\"Avg matched tokens per program: \", num_matched/len(base_sents))\n",
    "    print(\"Total failed programs: \", total_failed)\n",
    "    jcc, acc = get_acc(info_lst)\n",
    "    return info_lst, failed_lst, num_matched, jcc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram:  5\n",
      "Previous skip layers:  5\n",
      "New skip layers:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.09803921568627451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.2916666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:09,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.2391304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.11538461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:18,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.11904761904761904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:22,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.12121212121212122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:24,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.14285714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:28,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.5757575757575758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:31,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed accuracy:  0.09803921568627451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:32,  3.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m skip \u001b[38;5;129;01min\u001b[39;00m skip_up_to:\n\u001b[1;32m     13\u001b[0m     info_lst[n][skip] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mngram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_sents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphrases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     info_lst[n][skip][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     info_lst[n][skip][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 99\u001b[0m, in \u001b[0;36mngram\u001b[0;34m(n, model_copy, model, skip_up_to, max_steps, extra_steps, k, base_sents, phrases)\u001b[0m\n\u001b[1;32m     96\u001b[0m     elapsed_orig \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# store info\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     dict_pred_info[step_i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtopk_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     dict_pred_info[step_i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m elapsed_orig\n\u001b[1;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mcache_partial\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "extra_steps = 30\n",
    "max_steps = 1024\n",
    "k=100\n",
    "seed_everything(seed)\n",
    "\n",
    "ns = [5]\n",
    "skip_up_to = [40]\n",
    "info_lst = {}\n",
    "for n in ns:\n",
    "    info_lst[n] = {}\n",
    "    for skip in skip_up_to:\n",
    "        info_lst[n][skip] = {}\n",
    "        outputs = ngram(n, model_copy, model, skip, max_steps, extra_steps, k, base_sents, phrases)\n",
    "        info_lst[n][skip]['info'] = outputs[0]\n",
    "        info_lst[n][skip]['failed'] = outputs[1]\n",
    "        info_lst[n][skip]['num_matched'] = outputs[2]\n",
    "        info_lst[n][skip]['jcc'] = outputs[3]\n",
    "        info_lst[n][skip]['acc'] = outputs[4]\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "subset = load_from_disk(\"english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:1000]\n",
    "phrases = subset['train']['phrase'][:1000]\n",
    "edited_sents = subset['train']['edited_sentence'][:1000]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
