{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x, return_qkv):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        if return_qkv:\n",
    "            # If we want to store q, k, v for some advanced usage\n",
    "            # return them in the original shape\n",
    "            return y, (q, k, v)\n",
    "        else:\n",
    "            return y, None\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, matching_indices_1st = None, matching_indices_2nd = None):\n",
    "        x = x + self.attn(self.ln_1(x), matching_indices_1st, matching_indices_2nd)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte':  nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe':  nn.Embedding(config.block_size, config.n_embd),\n",
    "            'h':    nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # This cache holds precomputed states for skipping\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        batch_idx: int,\n",
    "        t_new: int,\n",
    "        t_matched: int = None,\n",
    "        skip_up_to: int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_ids: shape (B, T)\n",
    "        batch_idx: which batch index\n",
    "        t_new:     the new token position index we're computing\n",
    "        t_matched: if copying from a matched position\n",
    "        skip_up_to: how many layers to skip (None = no skip)\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # token + position embedding\n",
    "        pos = torch.arange(0, T, device=device)\n",
    "        x = self.transformer['wte'](input_ids) + self.transformer['wpe'](pos)\n",
    "        # run or skip layers\n",
    "        for layer_idx, block in enumerate(self.transformer['h']):\n",
    "            if (t_matched is not None) and (skip_up_to is not None) and (layer_idx < skip_up_to):\n",
    "                # skip: reuse hidden from the matched position\n",
    "                x = self.cache[(batch_idx, t_matched)]['hidden'][layer_idx]\n",
    "            else:\n",
    "                x_ln = block.ln_1(x)\n",
    "                attn_out, qkv = block.attn(x_ln, return_qkv=True)\n",
    "                x = x + attn_out\n",
    "                x = x + block.mlp(block.ln_2(x))\n",
    "\n",
    "                # store in cache\n",
    "                if (batch_idx, t_new) not in self.cache:\n",
    "                    self.cache[(batch_idx, t_new)] = {'hidden': {}, 'qkv': {}}\n",
    "\n",
    "                self.cache[(batch_idx, t_new)]['hidden'][layer_idx] = x.detach()\n",
    "                self.cache[(batch_idx, t_new)]['qkv'][layer_idx] = (qkv[0], qkv[1], qkv[2])\n",
    "\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_ngram_copy(seq_ids: torch.Tensor, n=3, skip_up_to=43):\n",
    "#     \"\"\"\n",
    "#     seq_ids : 1D tensor of shape (T,) for the current sequence of token IDs.\n",
    "#     n       : n-gram size (e.g. n=3 means we check for a 3-gram match).\n",
    "#     skip_up_to : how many layers to skip if a match is found (just a parameter).\n",
    "\n",
    "#     Returns:\n",
    "#       t_matched, skip_up_to  -> the matched position in seq_ids and how many layers to skip\n",
    "#       (None, None)           -> if no match is found\n",
    "#     \"\"\"\n",
    "\n",
    "#     T = seq_ids.size(0)\n",
    "#     if T < n:\n",
    "#         # Not enough tokens to form an n-gram\n",
    "#         return None, None\n",
    "\n",
    "#     # The very last token in the sequence\n",
    "#     last_token = seq_ids[-1].item()\n",
    "\n",
    "#     # Find all previous positions where last_token appears\n",
    "#     #    except the final position itself\n",
    "#     possible_positions = (seq_ids[:-1] == last_token).nonzero(as_tuple=True)[0]\n",
    "#     if len(possible_positions) == 0:\n",
    "#         # No earlier occurrence of this token\n",
    "#         return None, None\n",
    "\n",
    "#     # The (n-1) tokens before the last token\n",
    "#     #    e.g. if n=3, this is the last 2 tokens before the final one\n",
    "#     #    shape: (n-1,)\n",
    "#     context_needed = seq_ids[-(n-1):-1]\n",
    "\n",
    "#     # Scan from latest to earliest possible position\n",
    "#     matched_pos = None\n",
    "#     for pos in reversed(possible_positions):\n",
    "#         # pos is where the same last_token appeared,\n",
    "#         # but we also need to check if the (n-1) tokens before it\n",
    "#         # match 'context_needed'.\n",
    "\n",
    "#         # The matched token in seq_ids is at index 'pos'.\n",
    "#         # The (n-1) tokens before that are seq_ids[pos-(n-1)+1 : pos],\n",
    "#         # i.e. a total of (n-1) tokens ending just before 'pos'.\n",
    "#         # But simpler is to define start = pos - (n-1) + 1:\n",
    "#         start = pos - (n - 1) + 1\n",
    "#         if start < 0:\n",
    "#             # Not enough space in the sequence for an n-1 match\n",
    "#             continue\n",
    "\n",
    "#         # candidate (n-1) tokens that appear right before this matched token\n",
    "#         candidate = seq_ids[start:pos]\n",
    "\n",
    "#         # Compare with our needed context\n",
    "#         if torch.all(candidate == context_needed):\n",
    "#             matched_pos = pos\n",
    "#             break  # we use the latest match we find\n",
    "\n",
    "#     # 5) Return (matched_pos, skip_up_to) if found, else (None, None)\n",
    "#     if matched_pos is None:\n",
    "#         return None, None\n",
    "#     else:\n",
    "#         return matched_pos, skip_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_next_token(logits, top_k=5):\n",
    "#     \"\"\"\n",
    "#     Sample from the top-k tokens in the final position's logits.\n",
    "#     logits: (B, T, vocab_size)\n",
    "#     \"\"\"\n",
    "#     last_logits = logits[:, -1, :]  # shape (B, vocab_size)\n",
    "#     # top-k\n",
    "#     values, indices = torch.topk(last_logits, k=top_k, dim=-1)\n",
    "#     # sample from top-k\n",
    "#     probs = F.softmax(values, dim=-1)\n",
    "#     idx = torch.multinomial(probs, num_samples=1)  # shape (B, 1)\n",
    "#     next_token_id = indices.gather(-1, idx)       # shape (B, 1)\n",
    "#     return next_token_id.squeeze(-1)              # shape (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_input_ids(sent, tokenizer):\n",
    "    # Convert your sentence to input IDs\n",
    "    return tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    \n",
    "def get_top_k(logits, top_k):\n",
    "    prob = torch.softmax(logits, dim=-1)\n",
    "    top_values, top_indices = torch.topk(prob, top_k, dim=-1)\n",
    "    # Return top_indices[0] as the integer IDs (or a list of IDs)\n",
    "    return top_indices[0].tolist()  # list of int IDs\n",
    "\n",
    "def detect_ngram_copy(seq_ids: torch.Tensor, n=3, skip_up_to=43):\n",
    "    \"\"\"\n",
    "    Minimal function that tries to find n-gram copy scenario\n",
    "    (just a placeholder â€“ adapt to your real logic)\n",
    "    \"\"\"\n",
    "    T = seq_ids.size(1)  # shape (B=1, T)\n",
    "    if T < n:\n",
    "        return None, None\n",
    "    # 1) last token\n",
    "    last_token = seq_ids[0, -1].item()\n",
    "    # 2) find earlier positions of last_token\n",
    "    possible_pos = (seq_ids[0, :-1] == last_token).nonzero().view(-1)\n",
    "    if possible_pos.numel() == 0:\n",
    "        return None, None\n",
    "    # 3) check (n-1) context\n",
    "    n_minus_1 = n - 1\n",
    "    context_needed = seq_ids[0, -(n_minus_1+1):-1]  # last n-1 tokens\n",
    "    matched_pos = None\n",
    "    for pos in reversed(possible_pos):\n",
    "        if pos >= n_minus_1:\n",
    "            candidate = seq_ids[0, pos-n_minus_1:pos]\n",
    "            if torch.all(candidate == context_needed):\n",
    "                matched_pos = pos.item()\n",
    "                break\n",
    "    if matched_pos is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        return matched_pos, skip_up_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2-xl'\n",
    "model = GPT.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset from disk\n",
    "subset = load_from_disk(\"english_insertions\")\n",
    "prompt_list = []\n",
    "\n",
    "base_sents = subset['train']['base_sentence'][:1000]\n",
    "phrases = subset['train']['phrase'][:1000]\n",
    "edited_sents = subset['train']['edited_sentence'][:1000]\n",
    "\n",
    "import gc\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20        # number of tokens to generate\n",
    "reps = 5          # number of runs with different seeds\n",
    "k=10\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    prompt = ''\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    dict_pred_info = {}\n",
    "\n",
    "    for rep in range(reps):\n",
    "        dict_pred_info[rep] = {}\n",
    "        seed_everything(rep)\n",
    "\n",
    "        # 1) Copy model generation\n",
    "        copy_ids = input_ids.clone()\n",
    "        for step_i in range(steps):\n",
    "            t0 = time.time()\n",
    "\n",
    "            # detect copy scenario (toy example)\n",
    "            t_matched, skip_up_to = detect_ngram_copy(copy_ids, n=3, skip_up_to=43)\n",
    "\n",
    "            # forward pass\n",
    "            logits = model.forward(input_ids=copy_ids, \n",
    "                                    batch_idx=0, \n",
    "                                    t_new= copy_ids.shape[1] - 1, \n",
    "                                    t_matched=t_matched, \n",
    "                                    skip_up_to=skip_up_to)\n",
    "            top_k_list = get_top_k(logits, k=k, tokenizer=tokenizer)\n",
    "\n",
    "            # store in dictionary\n",
    "            if step_i not in dict_pred_info[rep]:\n",
    "                dict_pred_info[rep][step_i] = {}\n",
    "            dict_pred_info[rep][step_i]['copy'] = top_k_list\n",
    "\n",
    "            # pick next token\n",
    "            next_token = top_k_list[0]  # top-1 ID\n",
    "            copy_ids = torch.cat([copy_ids, torch.tensor([[next_token]])], dim=1)\n",
    "\n",
    "            elapsed_copy = time.time() - t0\n",
    "\n",
    "            dict_pred_info[rep][step_i]['copy_time'] = elapsed_copy\n",
    "\n",
    "        # 2) Original model generation\n",
    "        original_ids = input_ids.clone()\n",
    "        for step_i in range(steps):\n",
    "            t0 = time.time()\n",
    "\n",
    "            # no skip logic\n",
    "            logits = model.forward(input_ids=original_ids,\n",
    "                                    t_new = original_ids.shape[1] - 1\n",
    "                                    batch_idx=0)\n",
    "            top_k_list = get_top_k(logits, k=k, tokenizer=tokenizer)\n",
    "\n",
    "            dict_pred_info[rep][step_i]['original'] = top_k_list\n",
    "\n",
    "            # pick next token\n",
    "            next_token = top_k_list[0]  # top-1 ID\n",
    "            original_ids = torch.cat([original_ids, torch.tensor([[next_token]])], dim=1)\n",
    "\n",
    "            elapsed_orig = time.time() - t0\n",
    "\n",
    "            dict_pred_info[rep][step_i]['original_time'] = elapsed_orig\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copy_mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
